{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This Assignment will evaluate your skills in web scraping and text representation using\n",
        "Word2vec. Include in your submission the code used to generate answers as a Jupyter Notebook\n",
        "or Python program file as well as any files generated. Make sure it is clear what code answers\n",
        "each question.\n",
        "This Assignment is meant to be completed individually. You may discuss the questions at a high\n",
        "level with other students but the final work submitted must be your own. Please reference any\n",
        "external resources you use to complete this Assignment using ACM referencing format."
      ],
      "metadata": {
        "id": "RZHahU0LOxOB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 1"
      ],
      "metadata": {
        "id": "3ETWuwdJPoLR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this exercise, you will be scraping information from Books to Scrape. Write a function to\n",
        "extract the book titles, price, stock availability, and rating of all the books on the first 20 pages\n",
        "of the site. Write the results into a table that collects the information as separate columns."
      ],
      "metadata": {
        "id": "_-3SlgxQPs0x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "url = \"http://books.toscrape.com/catalogue/category/books_1/index.html\"\n",
        "\n",
        "# create an empty list to store the book information\n",
        "book_info = []\n",
        "\n",
        "# loop through the first 20 pages of the website\n",
        "for i in range(1, 21):\n",
        "    page_url = f\"http://books.toscrape.com/catalogue/category/books_1/page-{i}.html\"\n",
        "    response = requests.get(page_url)\n",
        "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "    books = soup.find_all(\"article\", class_=\"product_pod\")\n",
        "\n",
        "    # loop through each book on the page and extract the relevant information\n",
        "    for book in books:\n",
        "        title = book.h3.a[\"title\"]\n",
        "        price = book.select(\".price_color\")[0].get_text()[1:]\n",
        "        availability = book.select(\".availability\")[0].get_text().strip()\n",
        "        rating = book.select(\"p.star-rating\")[0].get(\"class\")[1]\n",
        "        book_info.append((title, price, availability, rating))\n",
        "\n",
        "# create a table from the extracted information using Pandas\n",
        "df = pd.DataFrame(book_info, columns=[\"Title\", \"Price\", \"Availability\", \"Rating\"])\n",
        "display(df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "Z2b_FFr9m1nv",
        "outputId": "1675f98a-01bf-4974-f242-31bff92d59dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                                                 Title   Price Availability  \\\n",
              "0                                 A Light in the Attic  £51.77     In stock   \n",
              "1                                   Tipping the Velvet  £53.74     In stock   \n",
              "2                                           Soumission  £50.10     In stock   \n",
              "3                                        Sharp Objects  £47.82     In stock   \n",
              "4                Sapiens: A Brief History of Humankind  £54.23     In stock   \n",
              "..                                                 ...     ...          ...   \n",
              "395        Take Me Home Tonight (Rock Star Romance #3)  £53.98     In stock   \n",
              "396                  Sleeping Giants (Themis Files #1)  £48.74     In stock   \n",
              "397  Setting the World on Fire: The Brief, Astonish...  £21.15     In stock   \n",
              "398                                  Playing with Fire  £13.71     In stock   \n",
              "399              Off the Hook (Fishing for Trouble #1)  £47.67     In stock   \n",
              "\n",
              "    Rating  \n",
              "0    Three  \n",
              "1      One  \n",
              "2      One  \n",
              "3     Four  \n",
              "4     Five  \n",
              "..     ...  \n",
              "395  Three  \n",
              "396    One  \n",
              "397    Two  \n",
              "398  Three  \n",
              "399  Three  \n",
              "\n",
              "[400 rows x 4 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-12252902-3839-4a37-8141-0ca43645cd67\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Title</th>\n",
              "      <th>Price</th>\n",
              "      <th>Availability</th>\n",
              "      <th>Rating</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>A Light in the Attic</td>\n",
              "      <td>£51.77</td>\n",
              "      <td>In stock</td>\n",
              "      <td>Three</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Tipping the Velvet</td>\n",
              "      <td>£53.74</td>\n",
              "      <td>In stock</td>\n",
              "      <td>One</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Soumission</td>\n",
              "      <td>£50.10</td>\n",
              "      <td>In stock</td>\n",
              "      <td>One</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Sharp Objects</td>\n",
              "      <td>£47.82</td>\n",
              "      <td>In stock</td>\n",
              "      <td>Four</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Sapiens: A Brief History of Humankind</td>\n",
              "      <td>£54.23</td>\n",
              "      <td>In stock</td>\n",
              "      <td>Five</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>395</th>\n",
              "      <td>Take Me Home Tonight (Rock Star Romance #3)</td>\n",
              "      <td>£53.98</td>\n",
              "      <td>In stock</td>\n",
              "      <td>Three</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>396</th>\n",
              "      <td>Sleeping Giants (Themis Files #1)</td>\n",
              "      <td>£48.74</td>\n",
              "      <td>In stock</td>\n",
              "      <td>One</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>397</th>\n",
              "      <td>Setting the World on Fire: The Brief, Astonish...</td>\n",
              "      <td>£21.15</td>\n",
              "      <td>In stock</td>\n",
              "      <td>Two</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>398</th>\n",
              "      <td>Playing with Fire</td>\n",
              "      <td>£13.71</td>\n",
              "      <td>In stock</td>\n",
              "      <td>Three</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>399</th>\n",
              "      <td>Off the Hook (Fishing for Trouble #1)</td>\n",
              "      <td>£47.67</td>\n",
              "      <td>In stock</td>\n",
              "      <td>Three</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>400 rows × 4 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-12252902-3839-4a37-8141-0ca43645cd67')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-12252902-3839-4a37-8141-0ca43645cd67 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-12252902-3839-4a37-8141-0ca43645cd67');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 2"
      ],
      "metadata": {
        "id": "CKnOH0iVPzJy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir ex2"
      ],
      "metadata": {
        "id": "mrbQcoJ97R3P"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Read the reviews in IMDB_Dataset.csv. Convert the table of reviews into a bag-of-words\n",
        "(BOW) matrix using scikit-learn or some other method."
      ],
      "metadata": {
        "id": "2aYTLlqqP6Y_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "file = open(\"/content/ex2/IMDB_Dataset.csv\", \"r\")\n",
        "data = list(csv.reader(file, delimiter=\",\"))\n"
      ],
      "metadata": {
        "id": "-VlQhmX-9h4y"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "[row[0] for row in data]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U4iEMBTT-F2s",
        "outputId": "f551b02f-3011-4a30-e204-2225db8afd32"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['review',\n",
              " \"One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They are right, as this is exactly what happened with me.<br /><br />The first thing that struck me about Oz was its brutality and unflinching scenes of violence, which set in right from the word GO. Trust me, this is not a show for the faint hearted or timid. This show pulls no punches with regards to drugs, sex or violence. Its is hardcore, in the classic use of the word.<br /><br />It is called OZ as that is the nickname given to the Oswald Maximum Security State Penitentary. It focuses mainly on Emerald City, an experimental section of the prison where all the cells have glass fronts and face inwards, so privacy is not high on the agenda. Em City is home to many..Aryans, Muslims, gangstas, Latinos, Christians, Italians, Irish and more....so scuffles, death stares, dodgy dealings and shady agreements are never far away.<br /><br />I would say the main appeal of the show is due to the fact that it goes where other shows wouldn't dare. Forget pretty pictures painted for mainstream audiences, forget charm, forget romance...OZ doesn't mess around. The first episode I ever saw struck me as so nasty it was surreal, I couldn't say I was ready for it, but as I watched more, I developed a taste for Oz, and got accustomed to the high levels of graphic violence. Not just violence, but injustice (crooked guards who'll be sold out for a nickel, inmates who'll kill on order and get away with it, well mannered, middle class inmates being turned into prison bitches due to their lack of street skills or prison experience) Watching Oz, you may become comfortable with what is uncomfortable viewing....thats if you can get in touch with your darker side.\",\n",
              " 'A wonderful little production. <br /><br />The filming technique is very unassuming- very old-time-BBC fashion and gives a comforting, and sometimes discomforting, sense of realism to the entire piece. <br /><br />The actors are extremely well chosen- Michael Sheen not only \"has got all the polari\" but he has all the voices down pat too! You can truly see the seamless editing guided by the references to Williams\\' diary entries, not only is it well worth the watching but it is a terrificly written and performed piece. A masterful production about one of the great master\\'s of comedy and his life. <br /><br />The realism really comes home with the little things: the fantasy of the guard which, rather than use the traditional \\'dream\\' techniques remains solid then disappears. It plays on our knowledge and our senses, particularly with the scenes concerning Orton and Halliwell and the sets (particularly of their flat with Halliwell\\'s murals decorating every surface) are terribly well done.',\n",
              " 'I thought this was a wonderful way to spend time on a too hot summer weekend, sitting in the air conditioned theater and watching a light-hearted comedy. The plot is simplistic, but the dialogue is witty and the characters are likable (even the well bread suspected serial killer). While some may be disappointed when they realize this is not Match Point 2: Risk Addiction, I thought it was proof that Woody Allen is still fully in control of the style many of us have grown to love.<br /><br />This was the most I\\'d laughed at one of Woody\\'s comedies in years (dare I say a decade?). While I\\'ve never been impressed with Scarlet Johanson, in this she managed to tone down her \"sexy\" image and jumped right into a average, but spirited young woman.<br /><br />This may not be the crown jewel of his career, but it was wittier than \"Devil Wears Prada\" and more interesting than \"Superman\" a great comedy to go see with friends.',\n",
              " \"Basically there's a family where a little boy (Jake) thinks there's a zombie in his closet & his parents are fighting all the time.<br /><br />This movie is slower than a soap opera... and suddenly, Jake decides to become Rambo and kill the zombie.<br /><br />OK, first of all when you're going to make a film you must Decide if its a thriller or a drama! As a drama the movie is watchable. Parents are divorcing & arguing like in real life. And then we have Jake with his closet which totally ruins all the film! I expected to see a BOOGEYMAN similar movie, and instead i watched a drama with some meaningless thriller spots.<br /><br />3 out of 10 just for the well playing parents & descent dialogs. As for the shots with Jake: just ignore them.\",\n",
              " 'Petter Mattei\\'s \"Love in the Time of Money\" is a visually stunning film to watch. Mr. Mattei offers us a vivid portrait about human relations. This is a movie that seems to be telling us what money, power and success do to people in the different situations we encounter. <br /><br />This being a variation on the Arthur Schnitzler\\'s play about the same theme, the director transfers the action to the present time New York where all these different characters meet and connect. Each one is connected in one way, or another to the next person, but no one seems to know the previous point of contact. Stylishly, the film has a sophisticated luxurious look. We are taken to see how these people live and the world they live in their own habitat.<br /><br />The only thing one gets out of all these souls in the picture is the different stages of loneliness each one inhabits. A big city is not exactly the best place in which human relations find sincere fulfillment, as one discerns is the case with most of the people we encounter.<br /><br />The acting is good under Mr. Mattei\\'s direction. Steve Buscemi, Rosario Dawson, Carol Kane, Michael Imperioli, Adrian Grenier, and the rest of the talented cast, make these characters come alive.<br /><br />We wish Mr. Mattei good luck and await anxiously for his next work.',\n",
              " 'Probably my all-time favorite movie, a story of selflessness, sacrifice and dedication to a noble cause, but it\\'s not preachy or boring. It just never gets old, despite my having seen it some 15 or more times in the last 25 years. Paul Lukas\\' performance brings tears to my eyes, and Bette Davis, in one of her very few truly sympathetic roles, is a delight. The kids are, as grandma says, more like \"dressed-up midgets\" than children, but that only makes them more fun to watch. And the mother\\'s slow awakening to what\\'s happening in the world and under her own roof is believable and startling. If I had a dozen thumbs, they\\'d all be \"up\" for this movie.',\n",
              " \"I sure would like to see a resurrection of a up dated Seahunt series with the tech they have today it would bring back the kid excitement in me.I grew up on black and white TV and Seahunt with Gunsmoke were my hero's every week.You have my vote for a comeback of a new sea hunt.We need a change of pace in TV and this would work for a world of under water adventure.Oh by the way thank you for an outlet like this to view many viewpoints about TV and the many movies.So any ole way I believe I've got what I wanna say.Would be nice to read some more plus points about sea hunt.If my rhymes would be 10 lines would you let me submit,or leave me out to be in doubt and have me to quit,If this is so then I must go so lets do it.\",\n",
              " \"This show was an amazing, fresh & innovative idea in the 70's when it first aired. The first 7 or 8 years were brilliant, but things dropped off after that. By 1990, the show was not really funny anymore, and it's continued its decline further to the complete waste of time it is today.<br /><br />It's truly disgraceful how far this show has fallen. The writing is painfully bad, the performances are almost as bad - if not for the mildly entertaining respite of the guest-hosts, this show probably wouldn't still be on the air. I find it so hard to believe that the same creator that hand-selected the original cast also chose the band of hacks that followed. How can one recognize such brilliance and then see fit to replace it with such mediocrity? I felt I must give 2 stars out of respect for the original cast that made this show such a huge success. As it is now, the show is just awful. I can't believe it's still on the air.\",\n",
              " \"Encouraged by the positive comments about this film on here I was looking forward to watching this film. Bad mistake. I've seen 950+ films and this is truly one of the worst of them - it's awful in almost every way: editing, pacing, storyline, 'acting,' soundtrack (the film's only song - a lame country tune - is played no less than four times). The film looks cheap and nasty and is boring in the extreme. Rarely have I been so happy to see the end credits of a film. <br /><br />The only thing that prevents me giving this a 1-score is Harvey Keitel - while this is far from his best performance he at least seems to be making a bit of an effort. One for Keitel obsessives only.\",\n",
              " 'If you like original gut wrenching laughter you will like this movie. If you are young or old then you will love this movie, hell even my mom liked it.<br /><br />Great Camp!!!']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# let's do a little preprocessing by lowercasing and getting rid of punctuation\n",
        "x = [row[0] for row in data]\n",
        "processed_docs = [doc.lower().replace(\".\",\"\") for doc in x]\n",
        "\n",
        "#look at the documents list\n",
        "print(\"Our corpus: \", processed_docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZaCLRU-X7dMM",
        "outputId": "b093e4aa-6cf2-436c-c5c1-b562a5f1e865"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Our corpus:  ['review', \"one of the other reviewers has mentioned that after watching just 1 oz episode you'll be hooked they are right, as this is exactly what happened with me<br /><br />the first thing that struck me about oz was its brutality and unflinching scenes of violence, which set in right from the word go trust me, this is not a show for the faint hearted or timid this show pulls no punches with regards to drugs, sex or violence its is hardcore, in the classic use of the word<br /><br />it is called oz as that is the nickname given to the oswald maximum security state penitentary it focuses mainly on emerald city, an experimental section of the prison where all the cells have glass fronts and face inwards, so privacy is not high on the agenda em city is home to manyaryans, muslims, gangstas, latinos, christians, italians, irish and moreso scuffles, death stares, dodgy dealings and shady agreements are never far away<br /><br />i would say the main appeal of the show is due to the fact that it goes where other shows wouldn't dare forget pretty pictures painted for mainstream audiences, forget charm, forget romanceoz doesn't mess around the first episode i ever saw struck me as so nasty it was surreal, i couldn't say i was ready for it, but as i watched more, i developed a taste for oz, and got accustomed to the high levels of graphic violence not just violence, but injustice (crooked guards who'll be sold out for a nickel, inmates who'll kill on order and get away with it, well mannered, middle class inmates being turned into prison bitches due to their lack of street skills or prison experience) watching oz, you may become comfortable with what is uncomfortable viewingthats if you can get in touch with your darker side\", 'a wonderful little production <br /><br />the filming technique is very unassuming- very old-time-bbc fashion and gives a comforting, and sometimes discomforting, sense of realism to the entire piece <br /><br />the actors are extremely well chosen- michael sheen not only \"has got all the polari\" but he has all the voices down pat too! you can truly see the seamless editing guided by the references to williams\\' diary entries, not only is it well worth the watching but it is a terrificly written and performed piece a masterful production about one of the great master\\'s of comedy and his life <br /><br />the realism really comes home with the little things: the fantasy of the guard which, rather than use the traditional \\'dream\\' techniques remains solid then disappears it plays on our knowledge and our senses, particularly with the scenes concerning orton and halliwell and the sets (particularly of their flat with halliwell\\'s murals decorating every surface) are terribly well done', 'i thought this was a wonderful way to spend time on a too hot summer weekend, sitting in the air conditioned theater and watching a light-hearted comedy the plot is simplistic, but the dialogue is witty and the characters are likable (even the well bread suspected serial killer) while some may be disappointed when they realize this is not match point 2: risk addiction, i thought it was proof that woody allen is still fully in control of the style many of us have grown to love<br /><br />this was the most i\\'d laughed at one of woody\\'s comedies in years (dare i say a decade?) while i\\'ve never been impressed with scarlet johanson, in this she managed to tone down her \"sexy\" image and jumped right into a average, but spirited young woman<br /><br />this may not be the crown jewel of his career, but it was wittier than \"devil wears prada\" and more interesting than \"superman\" a great comedy to go see with friends', \"basically there's a family where a little boy (jake) thinks there's a zombie in his closet & his parents are fighting all the time<br /><br />this movie is slower than a soap opera and suddenly, jake decides to become rambo and kill the zombie<br /><br />ok, first of all when you're going to make a film you must decide if its a thriller or a drama! as a drama the movie is watchable parents are divorcing & arguing like in real life and then we have jake with his closet which totally ruins all the film! i expected to see a boogeyman similar movie, and instead i watched a drama with some meaningless thriller spots<br /><br />3 out of 10 just for the well playing parents & descent dialogs as for the shots with jake: just ignore them\", 'petter mattei\\'s \"love in the time of money\" is a visually stunning film to watch mr mattei offers us a vivid portrait about human relations this is a movie that seems to be telling us what money, power and success do to people in the different situations we encounter <br /><br />this being a variation on the arthur schnitzler\\'s play about the same theme, the director transfers the action to the present time new york where all these different characters meet and connect each one is connected in one way, or another to the next person, but no one seems to know the previous point of contact stylishly, the film has a sophisticated luxurious look we are taken to see how these people live and the world they live in their own habitat<br /><br />the only thing one gets out of all these souls in the picture is the different stages of loneliness each one inhabits a big city is not exactly the best place in which human relations find sincere fulfillment, as one discerns is the case with most of the people we encounter<br /><br />the acting is good under mr mattei\\'s direction steve buscemi, rosario dawson, carol kane, michael imperioli, adrian grenier, and the rest of the talented cast, make these characters come alive<br /><br />we wish mr mattei good luck and await anxiously for his next work', 'probably my all-time favorite movie, a story of selflessness, sacrifice and dedication to a noble cause, but it\\'s not preachy or boring it just never gets old, despite my having seen it some 15 or more times in the last 25 years paul lukas\\' performance brings tears to my eyes, and bette davis, in one of her very few truly sympathetic roles, is a delight the kids are, as grandma says, more like \"dressed-up midgets\" than children, but that only makes them more fun to watch and the mother\\'s slow awakening to what\\'s happening in the world and under her own roof is believable and startling if i had a dozen thumbs, they\\'d all be \"up\" for this movie', \"i sure would like to see a resurrection of a up dated seahunt series with the tech they have today it would bring back the kid excitement in mei grew up on black and white tv and seahunt with gunsmoke were my hero's every weekyou have my vote for a comeback of a new sea huntwe need a change of pace in tv and this would work for a world of under water adventureoh by the way thank you for an outlet like this to view many viewpoints about tv and the many moviesso any ole way i believe i've got what i wanna saywould be nice to read some more plus points about sea huntif my rhymes would be 10 lines would you let me submit,or leave me out to be in doubt and have me to quit,if this is so then i must go so lets do it\", \"this show was an amazing, fresh & innovative idea in the 70's when it first aired the first 7 or 8 years were brilliant, but things dropped off after that by 1990, the show was not really funny anymore, and it's continued its decline further to the complete waste of time it is today<br /><br />it's truly disgraceful how far this show has fallen the writing is painfully bad, the performances are almost as bad - if not for the mildly entertaining respite of the guest-hosts, this show probably wouldn't still be on the air i find it so hard to believe that the same creator that hand-selected the original cast also chose the band of hacks that followed how can one recognize such brilliance and then see fit to replace it with such mediocrity? i felt i must give 2 stars out of respect for the original cast that made this show such a huge success as it is now, the show is just awful i can't believe it's still on the air\", \"encouraged by the positive comments about this film on here i was looking forward to watching this film bad mistake i've seen 950+ films and this is truly one of the worst of them - it's awful in almost every way: editing, pacing, storyline, 'acting,' soundtrack (the film's only song - a lame country tune - is played no less than four times) the film looks cheap and nasty and is boring in the extreme rarely have i been so happy to see the end credits of a film <br /><br />the only thing that prevents me giving this a 1-score is harvey keitel - while this is far from his best performance he at least seems to be making a bit of an effort one for keitel obsessives only\", 'if you like original gut wrenching laughter you will like this movie if you are young or old then you will love this movie, hell even my mom liked it<br /><br />great camp!!!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Initialize the count vectorizer object\n",
        "count_vect = CountVectorizer()\n",
        "\n",
        "# Build a BOW representation\n",
        "bow_rep = count_vect.fit_transform(processed_docs)\n",
        "\n",
        "# Print out the vocabulary\n",
        "print(\"Vocabulary\", count_vect.vocabulary_, \"\\n\")\n",
        "\n",
        "# print(\"Vocabulary index for cat\", count_vect.vocabulary_.get(\"cat\"), \"\\n\")\n",
        "\n",
        "#see the BOW rep for documents\n",
        "for ind in range(0,len(processed_docs)):\n",
        "    print(\"BoW representation for Document {}: \".format(ind), bow_rep[ind].toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XlIO_aTw_FWK",
        "outputId": "f20ad610-e2d5-4b88-f276-7f03f3975455"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary {'review': 476, 'one': 397, 'of': 390, 'the': 581, 'other': 405, 'reviewers': 477, 'has': 253, 'mentioned': 357, 'that': 580, 'after': 14, 'watching': 637, 'just': 297, 'oz': 410, 'episode': 171, 'you': 674, 'll': 326, 'be': 49, 'hooked': 265, 'they': 589, 'are': 32, 'right': 479, 'as': 36, 'this': 593, 'is': 289, 'exactly': 175, 'what': 646, 'happened': 247, 'with': 656, 'me': 352, 'br': 64, 'first': 199, 'thing': 590, 'struck': 553, 'about': 6, 'was': 632, 'its': 292, 'brutality': 70, 'and': 26, 'unflinching': 616, 'scenes': 493, 'violence': 626, 'which': 649, 'set': 511, 'in': 279, 'from': 210, 'word': 662, 'go': 225, 'trust': 609, 'not': 387, 'show': 519, 'for': 204, 'faint': 185, 'hearted': 257, 'or': 400, 'timid': 599, 'pulls': 453, 'no': 385, 'punches': 454, 'regards': 468, 'to': 600, 'drugs': 158, 'sex': 513, 'hardcore': 251, 'classic': 93, 'use': 619, 'it': 290, 'called': 74, 'nickname': 384, 'given': 221, 'oswald': 404, 'maximum': 350, 'security': 501, 'state': 547, 'penitentary': 419, 'focuses': 202, 'mainly': 337, 'on': 396, 'emerald': 164, 'city': 91, 'an': 25, 'experimental': 179, 'section': 500, 'prison': 448, 'where': 648, 'all': 20, 'cells': 82, 'have': 254, 'glass': 224, 'fronts': 211, 'face': 183, 'inwards': 287, 'so': 530, 'privacy': 449, 'high': 262, 'agenda': 15, 'em': 163, 'home': 264, 'manyaryans': 345, 'muslims': 374, 'gangstas': 217, 'latinos': 309, 'christians': 90, 'italians': 291, 'irish': 288, 'moreso': 367, 'scuffles': 496, 'death': 123, 'stares': 544, 'dodgy': 148, 'dealings': 122, 'shady': 515, 'agreements': 16, 'never': 379, 'far': 189, 'away': 42, 'would': 667, 'say': 489, 'main': 336, 'appeal': 31, 'due': 159, 'fact': 184, 'goes': 226, 'shows': 520, 'wouldn': 668, 'dare': 117, 'forget': 205, 'pretty': 445, 'pictures': 427, 'painted': 414, 'mainstream': 338, 'audiences': 38, 'charm': 85, 'romanceoz': 482, 'doesn': 149, 'mess': 358, 'around': 34, 'ever': 173, 'saw': 488, 'nasty': 377, 'surreal': 565, 'couldn': 111, 'ready': 461, 'but': 72, 'watched': 636, 'more': 366, 'developed': 133, 'taste': 570, 'got': 229, 'accustomed': 7, 'levels': 317, 'graphic': 231, 'injustice': 281, 'crooked': 115, 'guards': 237, 'who': 652, 'sold': 532, 'out': 407, 'nickel': 383, 'inmates': 282, 'kill': 302, 'order': 401, 'get': 218, 'well': 644, 'mannered': 343, 'middle': 360, 'class': 92, 'being': 52, 'turned': 611, 'into': 286, 'bitches': 59, 'their': 583, 'lack': 306, 'street': 552, 'skills': 527, 'experience': 178, 'may': 351, 'become': 50, 'comfortable': 100, 'uncomfortable': 614, 'viewingthats': 624, 'if': 274, 'can': 76, 'touch': 605, 'your': 676, 'darker': 118, 'side': 521, 'wonderful': 660, 'little': 324, 'production': 451, 'filming': 196, 'technique': 573, 'very': 622, 'unassuming': 613, 'old': 394, 'time': 597, 'bbc': 48, 'fashion': 190, 'gives': 222, 'comforting': 101, 'sometimes': 535, 'discomforting': 144, 'sense': 507, 'realism': 463, 'entire': 169, 'piece': 428, 'actors': 10, 'extremely': 181, 'chosen': 89, 'michael': 359, 'sheen': 517, 'only': 398, 'polari': 438, 'he': 256, 'voices': 629, 'down': 152, 'pat': 417, 'too': 603, 'truly': 608, 'see': 502, 'seamless': 499, 'editing': 161, 'guided': 239, 'by': 73, 'references': 467, 'williams': 654, 'diary': 137, 'entries': 170, 'worth': 666, 'terrificly': 577, 'written': 671, 'performed': 423, 'masterful': 347, 'great': 232, 'master': 346, 'comedy': 98, 'his': 263, 'life': 318, 'really': 465, 'comes': 99, 'things': 591, 'fantasy': 188, 'guard': 236, 'rather': 458, 'than': 578, 'traditional': 606, 'dream': 155, 'techniques': 574, 'remains': 470, 'solid': 533, 'then': 586, 'disappears': 141, 'plays': 433, 'our': 406, 'knowledge': 305, 'senses': 508, 'particularly': 416, 'concerning': 104, 'orton': 403, 'halliwell': 245, 'sets': 512, 'flat': 201, 'murals': 373, 'decorating': 128, 'every': 174, 'surface': 564, 'terribly': 576, 'done': 150, 'thought': 594, 'way': 639, 'spend': 540, 'hot': 267, 'summer': 561, 'weekend': 642, 'sitting': 525, 'air': 17, 'conditioned': 105, 'theater': 582, 'light': 319, 'plot': 434, 'simplistic': 523, 'dialogue': 136, 'witty': 658, 'characters': 84, 'likable': 320, 'even': 172, 'bread': 65, 'suspected': 566, 'serial': 509, 'killer': 303, 'while': 650, 'some': 534, 'disappointed': 142, 'when': 647, 'realize': 464, 'match': 348, 'point': 436, 'risk': 480, 'addiction': 11, 'proof': 452, 'woody': 661, 'allen': 21, 'still': 549, 'fully': 213, 'control': 110, 'style': 555, 'many': 344, 'us': 618, 'grown': 235, 'love': 331, 'most': 368, 'laughed': 310, 'at': 37, 'comedies': 97, 'years': 672, 'decade': 124, 've': 621, 'been': 51, 'impressed': 278, 'scarlet': 492, 'johanson': 295, 'she': 516, 'managed': 342, 'tone': 602, 'her': 259, 'sexy': 514, 'image': 276, 'jumped': 296, 'average': 39, 'spirited': 541, 'young': 675, 'woman': 659, 'crown': 116, 'jewel': 294, 'career': 77, 'wittier': 657, 'devil': 134, 'wears': 641, 'prada': 442, 'interesting': 285, 'superman': 562, 'friends': 209, 'basically': 47, 'there': 587, 'family': 187, 'boy': 63, 'jake': 293, 'thinks': 592, 'zombie': 677, 'closet': 94, 'parents': 415, 'fighting': 194, 'movie': 370, 'slower': 529, 'soap': 531, 'opera': 399, 'suddenly': 560, 'decides': 126, 'rambo': 456, 'ok': 393, 're': 459, 'going': 227, 'make': 339, 'film': 195, 'must': 375, 'decide': 125, 'thriller': 595, 'drama': 154, 'watchable': 635, 'divorcing': 146, 'arguing': 33, 'like': 321, 'real': 462, 'we': 640, 'totally': 604, 'ruins': 485, 'expected': 177, 'boogeyman': 61, 'similar': 522, 'instead': 284, 'meaningless': 353, 'spots': 542, '10': 0, 'playing': 432, 'descent': 131, 'dialogs': 135, 'shots': 518, 'ignore': 275, 'them': 584, 'petter': 425, 'mattei': 349, 'money': 365, 'visually': 627, 'stunning': 554, 'watch': 634, 'mr': 372, 'offers': 392, 'vivid': 628, 'portrait': 439, 'human': 270, 'relations': 469, 'seems': 503, 'telling': 575, 'power': 441, 'success': 558, 'do': 147, 'people': 420, 'different': 138, 'situations': 526, 'encounter': 165, 'variation': 620, 'arthur': 35, 'schnitzler': 494, 'play': 430, 'same': 487, 'theme': 585, 'director': 140, 'transfers': 607, 'action': 9, 'present': 444, 'new': 380, 'york': 673, 'these': 588, 'meet': 355, 'connect': 106, 'each': 160, 'connected': 107, 'another': 27, 'next': 381, 'person': 424, 'know': 304, 'previous': 447, 'contact': 108, 'stylishly': 556, 'sophisticated': 537, 'luxurious': 334, 'look': 328, 'taken': 568, 'how': 268, 'live': 325, 'world': 664, 'own': 409, 'habitat': 242, 'gets': 219, 'souls': 538, 'picture': 426, 'stages': 543, 'loneliness': 327, 'inhabits': 280, 'big': 57, 'best': 55, 'place': 429, 'find': 198, 'sincere': 524, 'fulfillment': 212, 'discerns': 143, 'case': 79, 'acting': 8, 'good': 228, 'under': 615, 'direction': 139, 'steve': 548, 'buscemi': 71, 'rosario': 484, 'dawson': 121, 'carol': 78, 'kane': 298, 'imperioli': 277, 'adrian': 12, 'grenier': 233, 'rest': 474, 'talented': 569, 'cast': 80, 'come': 95, 'alive': 19, 'wish': 655, 'luck': 332, 'await': 40, 'anxiously': 28, 'work': 663, 'probably': 450, 'my': 376, 'favorite': 191, 'story': 550, 'selflessness': 506, 'sacrifice': 486, 'dedication': 129, 'noble': 386, 'cause': 81, 'preachy': 443, 'boring': 62, 'despite': 132, 'having': 255, 'seen': 504, '15': 1, 'times': 598, 'last': 308, '25': 3, 'paul': 418, 'lukas': 333, 'performance': 421, 'brings': 69, 'tears': 571, 'eyes': 182, 'bette': 56, 'davis': 120, 'few': 193, 'sympathetic': 567, 'roles': 481, 'delight': 130, 'kids': 301, 'grandma': 230, 'says': 490, 'dressed': 156, 'up': 617, 'midgets': 361, 'children': 87, 'makes': 340, 'fun': 214, 'mother': 369, 'slow': 528, 'awakening': 41, 'happening': 248, 'roof': 483, 'believable': 53, 'startling': 546, 'had': 244, 'dozen': 153, 'thumbs': 596, 'sure': 563, 'resurrection': 475, 'dated': 119, 'seahunt': 498, 'series': 510, 'tech': 572, 'today': 601, 'bring': 68, 'back': 44, 'kid': 300, 'excitement': 176, 'mei': 356, 'grew': 234, 'black': 60, 'white': 651, 'tv': 612, 'gunsmoke': 240, 'were': 645, 'hero': 261, 'weekyou': 643, 'vote': 630, 'comeback': 96, 'sea': 497, 'huntwe': 272, 'need': 378, 'change': 83, 'pace': 411, 'water': 638, 'adventureoh': 13, 'thank': 579, 'outlet': 408, 'view': 623, 'viewpoints': 625, 'moviesso': 371, 'any': 29, 'ole': 395, 'believe': 54, 'wanna': 631, 'saywould': 491, 'nice': 382, 'read': 460, 'plus': 435, 'points': 437, 'huntif': 271, 'rhymes': 478, 'lines': 323, 'let': 315, 'submit': 557, 'leave': 313, 'doubt': 151, 'quit': 455, 'lets': 316, 'amazing': 24, 'fresh': 208, 'innovative': 283, 'idea': 273, '70': 4, 'aired': 18, 'brilliant': 67, 'dropped': 157, 'off': 391, '1990': 2, 'funny': 215, 'anymore': 30, 'continued': 109, 'decline': 127, 'further': 216, 'complete': 103, 'waste': 633, 'disgraceful': 145, 'fallen': 186, 'writing': 670, 'painfully': 413, 'bad': 45, 'performances': 422, 'almost': 22, 'mildly': 362, 'entertaining': 168, 'respite': 473, 'guest': 238, 'hosts': 266, 'hard': 250, 'creator': 113, 'hand': 246, 'selected': 505, 'original': 402, 'also': 23, 'chose': 88, 'band': 46, 'hacks': 243, 'followed': 203, 'recognize': 466, 'such': 559, 'brilliance': 66, 'fit': 200, 'replace': 471, 'mediocrity': 354, 'felt': 192, 'give': 220, 'stars': 545, 'respect': 472, 'made': 335, 'huge': 269, 'now': 388, 'awful': 43, 'encouraged': 166, 'positive': 440, 'comments': 102, 'here': 260, 'looking': 329, 'forward': 206, 'mistake': 363, '950': 5, 'films': 197, 'worst': 665, 'pacing': 412, 'storyline': 551, 'soundtrack': 539, 'song': 536, 'lame': 307, 'country': 112, 'tune': 610, 'played': 431, 'less': 314, 'four': 207, 'looks': 330, 'cheap': 86, 'extreme': 180, 'rarely': 457, 'happy': 249, 'end': 167, 'credits': 114, 'prevents': 446, 'giving': 223, 'score': 495, 'harvey': 252, 'keitel': 299, 'least': 312, 'making': 341, 'bit': 58, 'effort': 162, 'obsessives': 389, 'gut': 241, 'wrenching': 669, 'laughter': 311, 'will': 653, 'hell': 258, 'mom': 364, 'liked': 322, 'camp': 75} \n",
            "\n",
            "BoW representation for Document 0:  [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
            "BoW representation for Document 1:  [[ 0  0  0  0  0  0  1  1  0  0  0  0  0  0  1  1  1  0  0  0  1  0  0  0\n",
            "   0  1  6  0  0  0  0  1  2  0  1  0  4  0  1  0  0  0  2  0  0  0  0  0\n",
            "   0  2  1  0  1  0  0  0  0  0  0  1  0  0  0  0  6  0  0  0  0  0  1  0\n",
            "   2  0  1  0  1  0  0  0  0  0  1  0  0  1  0  0  0  0  1  2  1  1  0  0\n",
            "   0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  1  0  0  0  1  0  1  1  0\n",
            "   0  0  1  1  0  0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0  1  1  0  0  0  0  0  0  0  0  1  2  0  0  0  1  1  0  0  0\n",
            "   0  0  0  2  0  1  0  1  0  0  1  1  0  0  0  1  1  1  0  0  0  1  0  0\n",
            "   0  0  0  0  0  0  0  2  0  0  1  0  5  3  0  0  0  0  1  1  0  0  0  0\n",
            "   0  1  2  0  0  1  0  0  1  1  1  0  0  1  0  1  0  0  0  0  0  1  0  0\n",
            "   0  0  0  0  0  0  0  1  0  0  0  1  0  1  1  0  0  1  0  0  0  0  2  0\n",
            "   1  1  0  0  0  0  0  0  0  0  1  0  0  0  0  3  0  1  2  0  0  0  1  1\n",
            "   1  9  6  1  2  0  0  0  0  2  0  0  0  0  1  0  0  0  1  0  0  1  0  0\n",
            "   0  0  0  0  0  1  0  0  0  0  0  0  0  0  3  0  0  0  0  0  0  0  0  0\n",
            "   1  1  1  0  0  0  0  1  0  1  0  0  0  0  1  1  4  0  0  0  0  1  1  0\n",
            "   1  0  0  0  0  0  1  1  0  0  0  0  0  0  1  0  0  1  0  1  0  0  0  1\n",
            "   1  1  0  3  0  0  7  0  0  0  0  0  3  1  0  0  3  1  0  0  1  2  0  1\n",
            "   0  0  5  0  0  0  1  0  0  0  0  1  0  0  0  0  0  0  0  1  0  0  0  0\n",
            "   0  0  0  0  0  0  0  0  0  0  0  0  0  1  0  0  3  1  0  0  0  1  1  0\n",
            "   0  0  0  0  0  1  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0  1  0  2\n",
            "   0  0  1  0  0  0  0  0  1  2  0  0  0  1  0  0  1  0  0  0  1  1  0  0\n",
            "   0  0  0  0  0  0  0  1  0  1  0  1  0  0  0  3  1  1  0  0  0  0  0  1\n",
            "   0  0  2  0  1  0  0  0  0  0  0  0  0  0  0  0  1  0  0  1  0  0  0  0\n",
            "   1  2  0  0  0  0  0  0  0  0  0  0  0  1  0  0  0  0  1  0  0  0  0  0\n",
            "   0  0  0  0  4 16  0  1  0  0  0  0  0  1  1  0  0  3  0  0  0  0  0  1\n",
            "   6  0  0  0  0  1  0  0  0  1  0  1  0  0  1  0  1  0  0  1  0  0  0  0\n",
            "   1  0  4  0  0  0  0  0  3  0  0  0  1  2  0  0  0  0  0  0  1  0  2  0\n",
            "   2  1  0  0  2  0  0  0  5  0  0  0  0  0  2  0  0  0  0  1  1  0  0  0\n",
            "   0  0  3  0  1  0]]\n",
            "BoW representation for Document 2:  [[ 0  0  0  0  0  0  1  0  0  0  1  0  0  0  0  0  0  0  0  0  2  0  0  0\n",
            "   0  0  7  0  0  0  0  0  2  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  6  0  0  0  0  0  0  0\n",
            "   2  1  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  1  0  0  0  0  0  0\n",
            "   0  0  1  1  0  1  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0  1  0  0  0  1  0  0\n",
            "   1  0  0  0  0  0  1  0  1  0  0  1  0  0  0  0  0  1  0  0  0  0  0  0\n",
            "   0  1  1  0  0  0  1  0  0  0  0  0  0  1  0  0  0  0  0  0  1  0  1  0\n",
            "   0  0  0  0  1  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0  0  0  1  0  0  0  0  0  0  1  0  0  1  0  0  0  1  0  0  1\n",
            "   0  0  0  0  0  2  0  0  0  0  0  0  0  2  0  0  1  0  0  0  0  0  0  1\n",
            "   1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  3  3  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  0  0  0  0  0  0\n",
            "   0  0  0  0  0  0  1  0  0  0  0  0  2  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0  0  0  0  0  0  0  1  1  0  0  0  0  0  0  0  0  0  0  0  1\n",
            "   0  0  0  0  0  0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  2  0  0  5  0  0  0  1  0  1  1  2  0  0  0  0  1  0  0  2  0\n",
            "   0  0  0  0  0  0  0  0  2  1  0  0  0  0  0  1  0  0  0  0  2  0  0  0\n",
            "   0  1  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  2  0  0  0  0\n",
            "   0  0  1  0  0  0  0  2  0  1  0  1  0  0  1  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0  0  0  0  0  0  0  0  0  0  1  0  0  0  0  0  1  0  0  1  0\n",
            "   0  0  0  1  1  0  0  0  1  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0  0  1  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0  0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0  1  1  0\n",
            "   1  1  1  0  0 16  0  1  0  0  1  0  0  0  0  1  0  0  0  0  0  1  0  0\n",
            "   2  0  0  1  0  0  1  0  1  0  0  0  0  1  0  0  0  0  0  1  0  0  2  0\n",
            "   0  0  0  0  0  1  0  0  0  0  0  0  0  1  0  0  0  0  0  0  3  0  0  0\n",
            "   0  1  0  0  0  0  1  0  3  0  0  0  1  0  0  0  0  0  1  0  0  0  0  1\n",
            "   0  0  1  0  0  0]]\n",
            "BoW representation for Document 3:  [[0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 4 0 0 0 0 0 1 0 0 0\n",
            "  0 1 0 1 0 0 0 0 0 0 0 0 0 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 4 1 0 0 0 0 0 0\n",
            "  3 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0 0 0 0 0 0 1 0 0\n",
            "  0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0\n",
            "  0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0\n",
            "  0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 1 0 0 1 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 1 4 0 0 0 0 0 1 1 0\n",
            "  0 4 2 0 0 0 1 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0\n",
            "  0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 0 2 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 2 0 0 4 0 0 0 0 0\n",
            "  1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1 0\n",
            "  0 0 0 0 0 1 0 0 0 0 1 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
            "  1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
            "  0 0 2 0 1 8 1 0 0 0 0 0 0 1 0 0 0 5 2 0 0 1 0 0 4 0 1 1 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 4 0 0 0 0 1 0 1 0 1 1 0 1 0 0 1\n",
            "  0 0 2 0 0 0 0 0 2 1 1 1 1 2 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0]]\n",
            "BoW representation for Document 4:  [[1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0 0 0 0 0 4 0 0 0 0 0 2 1 0 0\n",
            "  2 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 1 6 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0\n",
            "  0 0 1 0 0 0 0 0 0 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
            "  0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 2 0 0 0 1 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 1 0 0 0 0 0 0 0 0 3 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 2 0 0 0 0 1 0 0 0\n",
            "  0 2 0 0 1 4 0 0 0 2 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0\n",
            "  1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 3 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 1 0 0\n",
            "  0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0\n",
            "  0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 1 0 0 6 0 0 1 0 1 2 0 0 0 0 1 1 0 2 0 1 0 0 3 0 0 0 1 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 0 1 0 0 1\n",
            "  1 1 0 0 0 0 0 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 2]]\n",
            "BoW representation for Document 5:  [[ 0  0  0  0  0  0  2  0  1  1  0  0  1  0  0  0  0  0  0  1  2  0  0  0\n",
            "   0  0  5  1  1  0  0  0  1  0  0  1  1  0  0  0  1  0  0  0  0  0  0  0\n",
            "   0  1  0  0  1  0  0  1  0  1  0  0  0  0  0  0  8  0  0  0  0  0  0  1\n",
            "   1  0  0  0  0  0  1  1  1  0  0  0  2  0  0  0  0  0  0  1  0  0  0  1\n",
            "   0  0  0  0  0  0  0  0  0  0  1  1  1  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  3  1  1  0  0  1\n",
            "   0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  2  0  0  0  0  2  0  0\n",
            "   0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  2  0  0  1  0  0  0  0  0  1  0  0  0  0  0  0  0  1  0  0  0\n",
            "   0  0  0  1  0  0  0  0  0  0  0  0  2  0  0  0  0  1  0  0  0  0  0  0\n",
            "   0  0  1  0  0  0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0  1\n",
            "   0  0  0  0  1  0  2  0  0  0  0  0  0  1  0  6  1  0  0  0  0  0  0  0\n",
            "   0  7  0  0  0  0  0  0  0  0  1  0  0  0  0  0  1  0  0  0  0  0  0  0\n",
            "   0  0  0  0  0  0  0  0  0  0  0  0  0  2  0  1  1  0  0  1  1  0  1  0\n",
            "   0  0  0  1  0  0  0  0  0  0  0  0  0  4  0  0  0  0  0  1  0  0  0  1\n",
            "   0  0  0  0  0  2  0  0  1  0  1  0  3  0  0  0  0  0  0  0  1  2  0  0\n",
            "   0  1  0  1  0  0  6  0  1  0  0  0  1  6  1  0  1  0  0  0  0  0  0  1\n",
            "   0  1  0  0  0  0  0  0  0  0  0  0  3  0  0  0  1  1  1  0  0  1  1  0\n",
            "   0  0  0  0  1  0  0  1  0  1  0  0  1  0  0  1  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0  0  0  0  0  0  0  0  0  0  2  0  0  0  0  1  0  0  0  0  0\n",
            "   0  0  0  0  1  0  0  1  0  0  0  0  0  0  1  0  0  0  0  0  0  0  1  2\n",
            "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  0  1  0\n",
            "   0  0  0  0  0  0  0  0  0  1  1  0  0  0  0  1  0  0  0  0  1  0  0  0\n",
            "   0  0  1  0  1  0  1  0  0  0  0  0  0  0  0  0  1  1  0  0  0  0  0  1\n",
            "   0  0  0  0  1 20  0  1  0  1  0  0  4  1  1  0  0  2  0  0  0  2  0  0\n",
            "   7  0  0  0  0  0  0  1  0  0  0  0  0  0  0  1  0  0  2  0  1  0  0  0\n",
            "   0  0  0  1  1  0  0  0  0  0  1  0  0  0  0  1  4  0  0  0  0  0  1  0\n",
            "   1  1  0  0  0  0  0  1  1  0  0  0  0  0  0  1  1  0  0  0  0  0  0  0\n",
            "   0  1  0  0  0  0]]\n",
            "BoW representation for Document 6:  [[0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 5 0 0 0 0 0 1 0 0 0\n",
            "  1 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0\n",
            "  2 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0\n",
            "  0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0\n",
            "  0 0 0 1 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 3 0 0 0 0 0 0 0 0\n",
            "  0 2 3 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
            "  0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 1 0 0 0 0 3 0 0 1 2 0 0 0 0 0 3 0 0 1 0 0 0 0 0 0 1 1 0 0 2 0 0 0 1 0\n",
            "  0 1 1 0 2 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0\n",
            "  0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0\n",
            "  0 0 1 0 1 4 0 0 1 0 0 0 0 1 0 0 0 1 0 0 1 1 1 0 4 0 0 0 0 0 0 0 1 0 0 0\n",
            "  0 0 0 1 0 2 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0]]\n",
            "BoW representation for Document 7:  [[1 0 0 0 0 0 2 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 5 0 0 1 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 1 0 0 0 0 3 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0\n",
            "  0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 3 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 3 0 0 0 0 0 0 0 0\n",
            "  0 1 2 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 2 0 1\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 3 0 0 0 1 0 0 0\n",
            "  0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 1 3 0 1 0 1 0 1 0 0 0 0 0 0 0 4 0 0 0 0 1\n",
            "  1 0 0 0 1 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 2 2 0 0 0 1 0\n",
            "  0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 1 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0\n",
            "  0 0 0 1 0 4 0 0 0 0 1 0 0 1 0 0 0 3 0 0 0 0 0 0 5 1 0 0 0 0 0 0 0 0 0 0\n",
            "  3 0 0 1 0 2 0 0 0 1 0 1 0 1 0 0 0 0 1 1 0 0 0 0 0 0 1 2 0 0 0 1 0 1 1 0\n",
            "  0 0 0 1 0 0 0 0 2 0 0 0 0 0 0 1 1 0 0 5 0 0 0 0 0 0 2 0 0 0]]\n",
            "BoW representation for Document 8:  [[ 0  0  1  0  1  0  0  0  0  0  0  0  0  0  1  0  0  2  1  0  0  0  1  1\n",
            "   1  1  2  0  0  0  1  0  1  0  0  0  2  0  0  0  0  0  0  1  0  2  1  0\n",
            "   0  1  0  0  0  0  2  0  0  0  0  0  0  0  0  0  2  0  1  1  0  0  0  0\n",
            "   1  1  0  0  2  0  0  0  2  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0\n",
            "   0  0  0  0  0  0  0  1  0  0  0  0  0  1  0  0  0  1  0  0  0  0  0  0\n",
            "   0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  1  0  0  0  0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0\n",
            "   1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  0  0  1  0  0\n",
            "   1  0  0  0  0  0  1  2  1  0  0  1  2  0  0  0  1  0  0  0  0  0  0  1\n",
            "   1  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  0\n",
            "   0  0  0  1  0  0  1  0  0  0  1  0  0  1  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  1  0  2  1  0  0  0  1  1  0  0  0  0  1  0  0  0  1  0  0  0  0\n",
            "   0  4  8  0  1  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1\n",
            "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  0  0  0  0  0\n",
            "   0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0\n",
            "   0  0  0  2  1  0  4  1  0  0  0  0  2  1  0  0  1  0  2  0  0  0  0  1\n",
            "   0  0  0  0  0  1  0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  0  0  0  0  0\n",
            "   0  0  0  0  0  0  0  0  0  1  1  0  0  0  0  1  1  1  0  0  0  0  0  0\n",
            "   0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  0\n",
            "   0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  6  0  0  0  0  0  0  0  0\n",
            "   0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  0  0  0  2  0  0\n",
            "   0  0  0  0  0  0  1  3  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0  5 15  0  0  0  0  1  0  0  0  0  1  0  4  0  0  0  1  0  0\n",
            "   3  1  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0  0  0  0  0  2  1  0  0  0  0  0  0  0  0  0  0  0  1  0  1\n",
            "   0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  1  0  1  0\n",
            "   1  0  0  0  0  0]]\n",
            "BoW representation for Document 9:  [[0 0 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 3 0 0 0 0 0 0 0 0 0\n",
            "  0 1 0 0 0 0 0 1 0 1 0 0 0 1 0 1 0 0 0 1 0 0 1 0 0 0 1 0 2 0 0 0 0 0 0 0\n",
            "  0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
            "  0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 0 0\n",
            "  1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 5 0 1 0 0 0 0 0 0 1 0 1 1 0 0 1 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
            "  1 0 1 0 1 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0\n",
            "  0 5 1 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0\n",
            "  0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 1 4 0 0 0 0 0\n",
            "  1 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1\n",
            "  0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1\n",
            "  1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 1\n",
            "  0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 1 0 1 7 0 0 1 0 0 0 0 0 1 0 0 5 0 0 0 0 1 0 3 0 0 0 0 0 0 0 1 0 1 0\n",
            "  0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0\n",
            "  0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
            "BoW representation for Document 10:  [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0\n",
            "  0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 2 1 0\n",
            "  0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 1 0 0 0 0 0 2 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
            "  0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 4 1 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df_bow = pd.DataFrame(bow_rep.toarray(),columns=count_vect.get_feature_names_out())\n",
        "\n",
        "display(df_bow)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "xPR44bgi_X-e",
        "outputId": "a4a54fd9-115c-491b-b623-ed144cce61b6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "    10  15  1990  25  70  950  about  accustomed  acting  action  ...  wouldn  \\\n",
              "0    0   0     0   0   0    0      0           0       0       0  ...       0   \n",
              "1    0   0     0   0   0    0      1           1       0       0  ...       1   \n",
              "2    0   0     0   0   0    0      1           0       0       0  ...       0   \n",
              "3    0   0     0   0   0    0      0           0       0       0  ...       0   \n",
              "4    1   0     0   0   0    0      0           0       0       0  ...       0   \n",
              "5    0   0     0   0   0    0      2           0       1       1  ...       0   \n",
              "6    0   1     0   1   0    0      0           0       0       0  ...       0   \n",
              "7    1   0     0   0   0    0      2           0       0       0  ...       0   \n",
              "8    0   0     1   0   1    0      0           0       0       0  ...       1   \n",
              "9    0   0     0   0   0    1      1           0       1       0  ...       0   \n",
              "10   0   0     0   0   0    0      0           0       0       0  ...       0   \n",
              "\n",
              "    wrenching  writing  written  years  york  you  young  your  zombie  \n",
              "0           0        0        0      0     0    0      0     0       0  \n",
              "1           0        0        0      0     0    3      0     1       0  \n",
              "2           0        0        1      0     0    1      0     0       0  \n",
              "3           0        0        0      1     0    0      1     0       0  \n",
              "4           0        0        0      0     0    2      0     0       2  \n",
              "5           0        0        0      0     1    0      0     0       0  \n",
              "6           0        0        0      1     0    0      0     0       0  \n",
              "7           0        0        0      0     0    2      0     0       0  \n",
              "8           0        1        0      1     0    0      0     0       0  \n",
              "9           0        0        0      0     0    0      0     0       0  \n",
              "10          1        0        0      0     0    4      1     0       0  \n",
              "\n",
              "[11 rows x 678 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-03b9079a-14f4-4118-b561-2ea81e0f1a42\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>10</th>\n",
              "      <th>15</th>\n",
              "      <th>1990</th>\n",
              "      <th>25</th>\n",
              "      <th>70</th>\n",
              "      <th>950</th>\n",
              "      <th>about</th>\n",
              "      <th>accustomed</th>\n",
              "      <th>acting</th>\n",
              "      <th>action</th>\n",
              "      <th>...</th>\n",
              "      <th>wouldn</th>\n",
              "      <th>wrenching</th>\n",
              "      <th>writing</th>\n",
              "      <th>written</th>\n",
              "      <th>years</th>\n",
              "      <th>york</th>\n",
              "      <th>you</th>\n",
              "      <th>young</th>\n",
              "      <th>your</th>\n",
              "      <th>zombie</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>11 rows × 678 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-03b9079a-14f4-4118-b561-2ea81e0f1a42')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-03b9079a-14f4-4118-b561-2ea81e0f1a42 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-03b9079a-14f4-4118-b561-2ea81e0f1a42');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Do the same thing for each bigram of the text."
      ],
      "metadata": {
        "id": "RfFySxwVP-Uj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# bigram vectorization example with count vectorizer bigrams\n",
        "count_vect = CountVectorizer(ngram_range=(2, 2))\n",
        "# Build a BOW representation for the corpus\n",
        "bow_rep = count_vect.fit_transform(processed_docs)\n",
        "\n",
        "# Vocabulary mapping\n",
        "print(\"Vocabulary: \", count_vect.vocabulary_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lJiRBrwhACiE",
        "outputId": "9d35c94b-b2ee-46d0-b923-2e79435bd5b1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary:  {'one of': 772, 'of the': 744, 'the other': 1090, 'other reviewers': 797, 'reviewers has': 891, 'has mentioned': 424, 'mentioned that': 656, 'that after': 1031, 'after watching': 23, 'watching just': 1270, 'just oz': 567, 'oz episode': 809, 'episode you': 298, 'you ll': 1369, 'll be': 607, 'be hooked': 126, 'hooked they': 454, 'they are': 1142, 'are right': 90, 'right as': 893, 'as this': 105, 'this is': 1153, 'is exactly': 506, 'exactly what': 306, 'what happened': 1295, 'happened with': 415, 'with me': 1325, 'me br': 646, 'br br': 156, 'br the': 161, 'the first': 1070, 'first thing': 345, 'thing that': 1147, 'that struck': 1041, 'struck me': 996, 'me about': 644, 'about oz': 9, 'oz was': 810, 'was its': 1254, 'its brutality': 552, 'brutality and': 170, 'and unflinching': 75, 'unflinching scenes': 1224, 'scenes of': 913, 'of violence': 751, 'violence which': 1247, 'which set': 1308, 'set in': 939, 'in right': 481, 'right from': 894, 'from the': 368, 'the word': 1118, 'word go': 1342, 'go trust': 387, 'trust me': 1215, 'me this': 650, 'is not': 515, 'not show': 723, 'show for': 947, 'for the': 358, 'the faint': 1066, 'faint hearted': 316, 'hearted or': 437, 'or timid': 789, 'timid this': 1176, 'this show': 1158, 'show pulls': 951, 'pulls no': 865, 'no punches': 712, 'punches with': 866, 'with regards': 1327, 'regards to': 882, 'to drugs': 1180, 'drugs sex': 283, 'sex or': 941, 'or violence': 790, 'violence its': 1245, 'its is': 554, 'is hardcore': 509, 'hardcore in': 419, 'in the': 482, 'the classic': 1056, 'classic use': 209, 'use of': 1232, 'word br': 1341, 'br it': 158, 'it is': 539, 'is called': 502, 'called oz': 183, 'oz as': 808, 'as that': 104, 'that is': 1035, 'is the': 524, 'the nickname': 1086, 'nickname given': 709, 'given to': 381, 'to the': 1194, 'the oswald': 1089, 'oswald maximum': 796, 'maximum security': 640, 'security state': 923, 'state penitentary': 988, 'penitentary it': 822, 'it focuses': 537, 'focuses mainly': 348, 'mainly on': 621, 'on emerald': 761, 'emerald city': 290, 'city an': 206, 'an experimental': 42, 'experimental section': 310, 'section of': 922, 'the prison': 1099, 'prison where': 858, 'where all': 1303, 'all the': 31, 'the cells': 1054, 'cells have': 195, 'have glass': 427, 'glass fronts': 384, 'fronts and': 369, 'and face': 48, 'face inwards': 314, 'inwards so': 498, 'so privacy': 967, 'privacy is': 859, 'not high': 717, 'high on': 445, 'on the': 765, 'the agenda': 1048, 'agenda em': 24, 'em city': 289, 'city is': 207, 'is home': 511, 'home to': 452, 'to manyaryans': 1185, 'manyaryans muslims': 632, 'muslims gangstas': 686, 'gangstas latinos': 375, 'latinos christians': 582, 'christians italians': 205, 'italians irish': 551, 'irish and': 499, 'and moreso': 61, 'moreso scuffles': 673, 'scuffles death': 916, 'death stares': 243, 'stares dodgy': 985, 'dodgy dealings': 271, 'dealings and': 242, 'and shady': 66, 'shady agreements': 943, 'agreements are': 25, 'are never': 89, 'never far': 701, 'far away': 320, 'away br': 113, 'br would': 164, 'would say': 1352, 'say the': 907, 'the main': 1079, 'main appeal': 620, 'appeal of': 82, 'the show': 1108, 'show is': 949, 'is due': 505, 'due to': 284, 'the fact': 1065, 'fact that': 315, 'that it': 1036, 'it goes': 538, 'goes where': 388, 'where other': 1305, 'other shows': 798, 'shows wouldn': 954, 'wouldn dare': 1355, 'dare forget': 236, 'forget pretty': 362, 'pretty pictures': 853, 'pictures painted': 833, 'painted for': 815, 'for mainstream': 355, 'mainstream audiences': 622, 'audiences forget': 109, 'forget charm': 361, 'charm forget': 200, 'forget romanceoz': 363, 'romanceoz doesn': 898, 'doesn mess': 272, 'mess around': 657, 'around the': 95, 'first episode': 342, 'episode ever': 297, 'ever saw': 301, 'saw struck': 905, 'me as': 645, 'as so': 103, 'so nasty': 966, 'nasty it': 698, 'it was': 547, 'was surreal': 1259, 'surreal couldn': 1011, 'couldn say': 230, 'say was': 908, 'was ready': 1258, 'ready for': 873, 'for it': 353, 'it but': 534, 'but as': 172, 'as watched': 106, 'watched more': 1268, 'more developed': 667, 'developed taste': 253, 'taste for': 1016, 'for oz': 357, 'oz and': 807, 'and got': 51, 'got accustomed': 392, 'accustomed to': 14, 'the high': 1074, 'high levels': 444, 'levels of': 590, 'of graphic': 732, 'graphic violence': 396, 'violence not': 1246, 'not just': 718, 'just violence': 568, 'violence but': 1244, 'but injustice': 174, 'injustice crooked': 490, 'crooked guards': 234, 'guards who': 404, 'who ll': 1314, 'be sold': 131, 'sold out': 970, 'out for': 801, 'for nickel': 356, 'nickel inmates': 708, 'inmates who': 492, 'll kill': 608, 'kill on': 574, 'on order': 763, 'order and': 792, 'and get': 49, 'get away': 376, 'away with': 114, 'with it': 1323, 'it well': 548, 'well mannered': 1290, 'mannered middle': 628, 'middle class': 660, 'class inmates': 208, 'inmates being': 491, 'being turned': 139, 'turned into': 1217, 'into prison': 497, 'prison bitches': 856, 'bitches due': 150, 'to their': 1195, 'their lack': 1125, 'lack of': 579, 'of street': 743, 'street skills': 995, 'skills or': 960, 'or prison': 788, 'prison experience': 857, 'experience watching': 309, 'watching oz': 1272, 'oz you': 811, 'you may': 1370, 'may become': 642, 'become comfortable': 135, 'comfortable with': 219, 'with what': 1332, 'what is': 1297, 'is uncomfortable': 527, 'uncomfortable viewingthats': 1220, 'viewingthats if': 1242, 'if you': 469, 'you can': 1365, 'can get': 185, 'get in': 377, 'in touch': 485, 'touch with': 1208, 'with your': 1333, 'your darker': 1376, 'darker side': 238, 'wonderful little': 1337, 'little production': 603, 'production br': 863, 'the filming': 1069, 'filming technique': 337, 'technique is': 1019, 'is very': 528, 'very unassuming': 1240, 'unassuming very': 1219, 'very old': 1239, 'old time': 758, 'time bbc': 1167, 'bbc fashion': 123, 'fashion and': 323, 'and gives': 50, 'gives comforting': 382, 'comforting and': 220, 'and sometimes': 67, 'sometimes discomforting': 976, 'discomforting sense': 266, 'sense of': 935, 'of realism': 740, 'realism to': 876, 'the entire': 1063, 'entire piece': 295, 'piece br': 834, 'the actors': 1047, 'actors are': 18, 'are extremely': 86, 'extremely well': 312, 'well chosen': 1288, 'chosen michael': 204, 'michael sheen': 659, 'sheen not': 945, 'not only': 720, 'only has': 776, 'has got': 423, 'got all': 393, 'the polari': 1095, 'polari but': 846, 'but he': 173, 'he has': 435, 'has all': 421, 'the voices': 1114, 'voices down': 1250, 'down pat': 275, 'pat too': 820, 'too you': 1206, 'can truly': 187, 'truly see': 1213, 'see the': 928, 'the seamless': 1105, 'seamless editing': 921, 'editing guided': 286, 'guided by': 406, 'by the': 182, 'the references': 1101, 'references to': 881, 'to williams': 1201, 'williams diary': 1317, 'diary entries': 257, 'entries not': 296, 'only is': 777, 'is it': 512, 'well worth': 1292, 'worth the': 1348, 'the watching': 1115, 'watching but': 1269, 'but it': 175, 'is terrificly': 523, 'terrificly written': 1023, 'written and': 1359, 'and performed': 64, 'performed piece': 829, 'piece masterful': 835, 'masterful production': 634, 'production about': 862, 'about one': 8, 'the great': 1071, 'great master': 399, 'master of': 633, 'of comedy': 729, 'comedy and': 215, 'and his': 54, 'his life': 449, 'life br': 592, 'the realism': 1100, 'realism really': 875, 'really comes': 878, 'comes home': 218, 'home with': 453, 'with the': 1331, 'the little': 1078, 'little things': 604, 'things the': 1149, 'the fantasy': 1067, 'fantasy of': 319, 'the guard': 1072, 'guard which': 403, 'which rather': 1307, 'rather than': 870, 'than use': 1029, 'use the': 1233, 'the traditional': 1113, 'traditional dream': 1209, 'dream techniques': 280, 'techniques remains': 1020, 'remains solid': 885, 'solid then': 971, 'then disappears': 1130, 'disappears it': 263, 'it plays': 542, 'plays on': 840, 'on our': 764, 'our knowledge': 799, 'knowledge and': 578, 'and our': 63, 'our senses': 800, 'senses particularly': 936, 'particularly with': 819, 'the scenes': 1104, 'scenes concerning': 912, 'concerning orton': 223, 'orton and': 795, 'and halliwell': 52, 'halliwell and': 412, 'and the': 71, 'the sets': 1106, 'sets particularly': 940, 'particularly of': 818, 'of their': 745, 'their flat': 1124, 'flat with': 347, 'with halliwell': 1321, 'halliwell murals': 413, 'murals decorating': 685, 'decorating every': 248, 'every surface': 302, 'surface are': 1010, 'are terribly': 92, 'terribly well': 1022, 'well done': 1289, 'thought this': 1163, 'this was': 1160, 'was wonderful': 1262, 'wonderful way': 1338, 'way to': 1279, 'to spend': 1193, 'spend time': 981, 'time on': 1173, 'on too': 766, 'too hot': 1205, 'hot summer': 456, 'summer weekend': 1007, 'weekend sitting': 1285, 'sitting in': 958, 'the air': 1049, 'air conditioned': 26, 'conditioned theater': 224, 'theater and': 1123, 'and watching': 76, 'watching light': 1271, 'light hearted': 593, 'hearted comedy': 436, 'comedy the': 216, 'the plot': 1094, 'plot is': 841, 'is simplistic': 519, 'simplistic but': 956, 'but the': 179, 'the dialogue': 1059, 'dialogue is': 256, 'is witty': 531, 'witty and': 1335, 'the characters': 1055, 'characters are': 197, 'are likable': 88, 'likable even': 594, 'even the': 300, 'the well': 1117, 'well bread': 1287, 'bread suspected': 165, 'suspected serial': 1012, 'serial killer': 937, 'killer while': 576, 'while some': 1310, 'some may': 973, 'may be': 641, 'be disappointed': 125, 'disappointed when': 264, 'when they': 1301, 'they realize': 1145, 'realize this': 877, 'not match': 719, 'match point': 635, 'point risk': 844, 'risk addiction': 896, 'addiction thought': 19, 'thought it': 1162, 'was proof': 1257, 'proof that': 864, 'that woody': 1043, 'woody allen': 1339, 'allen is': 35, 'is still': 522, 'still fully': 991, 'fully in': 371, 'in control': 475, 'control of': 229, 'the style': 1109, 'style many': 998, 'many of': 630, 'of us': 750, 'us have': 1229, 'have grown': 428, 'grown to': 402, 'to love': 1183, 'love br': 613, 'br this': 162, 'was the': 1260, 'the most': 1082, 'most laughed': 674, 'laughed at': 583, 'at one': 108, 'of woody': 752, 'woody comedies': 1340, 'comedies in': 214, 'in years': 488, 'years dare': 1360, 'dare say': 237, 'say decade': 906, 'decade while': 244, 'while ve': 1312, 've never': 1236, 'never been': 700, 'been impressed': 137, 'impressed with': 473, 'with scarlet': 1328, 'scarlet johanson': 911, 'johanson in': 561, 'in this': 484, 'this she': 1157, 'she managed': 944, 'managed to': 627, 'to tone': 1196, 'tone down': 1204, 'down her': 274, 'her sexy': 440, 'sexy image': 942, 'image and': 471, 'and jumped': 58, 'jumped right': 562, 'right into': 895, 'into average': 496, 'average but': 110, 'but spirited': 177, 'spirited young': 982, 'young woman': 1375, 'woman br': 1336, 'this may': 1154, 'may not': 643, 'not be': 714, 'be the': 133, 'the crown': 1058, 'crown jewel': 235, 'jewel of': 560, 'of his': 735, 'his career': 447, 'career but': 188, 'was wittier': 1261, 'wittier than': 1334, 'than devil': 1025, 'devil wears': 254, 'wears prada': 1284, 'prada and': 850, 'and more': 60, 'more interesting': 669, 'interesting than': 495, 'than superman': 1028, 'superman great': 1008, 'great comedy': 398, 'comedy to': 217, 'to go': 1181, 'go see': 385, 'see with': 929, 'with friends': 1319, 'basically there': 122, 'there family': 1135, 'family where': 318, 'where little': 1304, 'little boy': 602, 'boy jake': 155, 'jake thinks': 558, 'thinks there': 1150, 'there zombie': 1136, 'zombie in': 1378, 'in his': 477, 'his closet': 448, 'closet his': 210, 'his parents': 451, 'parents are': 816, 'are fighting': 87, 'fighting all': 327, 'the time': 1112, 'time br': 1168, 'this movie': 1155, 'movie is': 680, 'is slower': 520, 'slower than': 962, 'than soap': 1027, 'soap opera': 969, 'opera and': 781, 'and suddenly': 70, 'suddenly jake': 1006, 'jake decides': 556, 'decides to': 246, 'to become': 1178, 'become rambo': 136, 'rambo and': 868, 'and kill': 59, 'kill the': 575, 'the zombie': 1122, 'zombie br': 1377, 'br ok': 159, 'ok first': 755, 'first of': 343, 'of all': 727, 'all when': 34, 'when you': 1302, 'you re': 1372, 're going': 871, 'going to': 389, 'to make': 1184, 'make film': 623, 'film you': 336, 'you must': 1371, 'must decide': 687, 'decide if': 245, 'if its': 466, 'its thriller': 555, 'thriller or': 1164, 'or drama': 784, 'drama as': 277, 'as drama': 98, 'drama the': 278, 'the movie': 1084, 'is watchable': 530, 'watchable parents': 1266, 'are divorcing': 85, 'divorcing arguing': 268, 'arguing like': 94, 'like in': 596, 'in real': 480, 'real life': 874, 'life and': 591, 'and then': 72, 'then we': 1133, 'we have': 1282, 'have jake': 429, 'jake with': 559, 'with his': 1322, 'closet which': 211, 'which totally': 1309, 'totally ruins': 1207, 'ruins all': 901, 'the film': 1068, 'film expected': 330, 'expected to': 308, 'to see': 1192, 'see boogeyman': 924, 'boogeyman similar': 152, 'similar movie': 955, 'movie and': 677, 'and instead': 55, 'instead watched': 494, 'watched drama': 1267, 'drama with': 279, 'with some': 1329, 'some meaningless': 974, 'meaningless thriller': 652, 'thriller spots': 1165, 'spots br': 983, 'br out': 160, 'out of': 802, 'of 10': 726, '10 just': 0, 'just for': 564, 'well playing': 1291, 'playing parents': 839, 'parents descent': 817, 'descent dialogs': 251, 'dialogs as': 255, 'as for': 99, 'the shots': 1107, 'shots with': 946, 'with jake': 1324, 'jake just': 557, 'just ignore': 565, 'ignore them': 470, 'petter mattei': 831, 'mattei love': 638, 'love in': 614, 'time of': 1172, 'of money': 737, 'money is': 665, 'is visually': 529, 'visually stunning': 1248, 'stunning film': 997, 'film to': 335, 'to watch': 1198, 'watch mr': 1265, 'mr mattei': 684, 'mattei offers': 639, 'offers us': 754, 'us vivid': 1230, 'vivid portrait': 1249, 'portrait about': 847, 'about human': 7, 'human relations': 461, 'relations this': 884, 'is movie': 514, 'movie that': 682, 'that seems': 1040, 'seems to': 930, 'to be': 1177, 'be telling': 132, 'telling us': 1021, 'us what': 1231, 'what money': 1298, 'money power': 666, 'power and': 849, 'and success': 69, 'success do': 1002, 'do to': 270, 'to people': 1188, 'people in': 823, 'the different': 1060, 'different situations': 259, 'situations we': 959, 'we encounter': 1281, 'encounter br': 291, 'this being': 1151, 'being variation': 140, 'variation on': 1234, 'the arthur': 1050, 'arthur schnitzler': 96, 'schnitzler play': 914, 'play about': 837, 'about the': 11, 'the same': 1103, 'same theme': 904, 'theme the': 1129, 'the director': 1061, 'director transfers': 262, 'transfers the': 1210, 'the action': 1046, 'action to': 17, 'the present': 1097, 'present time': 852, 'time new': 1171, 'new york': 704, 'york where': 1363, 'all these': 32, 'these different': 1138, 'different characters': 258, 'characters meet': 199, 'meet and': 654, 'and connect': 46, 'connect each': 225, 'each one': 285, 'one is': 771, 'is connected': 503, 'connected in': 226, 'in one': 479, 'one way': 775, 'way or': 1277, 'or another': 782, 'another to': 78, 'the next': 1085, 'next person': 705, 'person but': 830, 'but no': 176, 'no one': 711, 'one seems': 774, 'to know': 1182, 'know the': 577, 'the previous': 1098, 'previous point': 855, 'point of': 843, 'of contact': 730, 'contact stylishly': 227, 'stylishly the': 999, 'film has': 331, 'has sophisticated': 425, 'sophisticated luxurious': 978, 'luxurious look': 618, 'look we': 610, 'we are': 1280, 'are taken': 91, 'taken to': 1014, 'see how': 926, 'how these': 459, 'these people': 1139, 'people live': 824, 'live and': 605, 'the world': 1119, 'world they': 1346, 'they live': 1144, 'live in': 606, 'in their': 483, 'their own': 1126, 'own habitat': 805, 'habitat br': 409, 'the only': 1087, 'only thing': 780, 'thing one': 1146, 'one gets': 769, 'gets out': 379, 'these souls': 1140, 'souls in': 979, 'the picture': 1093, 'picture is': 832, 'different stages': 260, 'stages of': 984, 'of loneliness': 736, 'loneliness each': 609, 'one inhabits': 770, 'inhabits big': 489, 'big city': 148, 'not exactly': 715, 'exactly the': 305, 'the best': 1052, 'best place': 146, 'place in': 836, 'in which': 487, 'which human': 1306, 'relations find': 883, 'find sincere': 340, 'sincere fulfillment': 957, 'fulfillment as': 370, 'as one': 102, 'one discerns': 767, 'discerns is': 265, 'the case': 1053, 'case with': 190, 'with most': 1326, 'most of': 675, 'the people': 1091, 'people we': 825, 'the acting': 1045, 'acting is': 15, 'is good': 508, 'good under': 391, 'under mr': 1222, 'mattei direction': 636, 'direction steve': 261, 'steve buscemi': 989, 'buscemi rosario': 171, 'rosario dawson': 900, 'dawson carol': 241, 'carol kane': 189, 'kane michael': 569, 'michael imperioli': 658, 'imperioli adrian': 472, 'adrian grenier': 20, 'grenier and': 400, 'the rest': 1102, 'rest of': 889, 'the talented': 1110, 'talented cast': 1015, 'cast make': 192, 'make these': 624, 'these characters': 1137, 'characters come': 198, 'come alive': 212, 'alive br': 29, 'br we': 163, 'we wish': 1283, 'wish mr': 1318, 'mattei good': 637, 'good luck': 390, 'luck and': 616, 'and await': 44, 'await anxiously': 111, 'anxiously for': 79, 'for his': 352, 'his next': 450, 'next work': 706, 'probably my': 860, 'my all': 690, 'all time': 33, 'time favorite': 1169, 'favorite movie': 324, 'movie story': 681, 'story of': 993, 'of selflessness': 742, 'selflessness sacrifice': 934, 'sacrifice and': 902, 'and dedication': 47, 'dedication to': 249, 'to noble': 1187, 'noble cause': 713, 'cause but': 194, 'it not': 541, 'not preachy': 721, 'preachy or': 851, 'or boring': 783, 'boring it': 154, 'it just': 540, 'just never': 566, 'never gets': 702, 'gets old': 378, 'old despite': 756, 'despite my': 252, 'my having': 692, 'having seen': 433, 'seen it': 932, 'it some': 544, 'some 15': 972, '15 or': 2, 'or more': 786, 'more times': 672, 'times in': 1174, 'the last': 1077, 'last 25': 581, '25 years': 4, 'years paul': 1361, 'paul lukas': 821, 'lukas performance': 617, 'performance brings': 826, 'brings tears': 169, 'tears to': 1017, 'to my': 1186, 'my eyes': 691, 'eyes and': 313, 'and bette': 45, 'bette davis': 147, 'davis in': 240, 'of her': 734, 'her very': 441, 'very few': 1238, 'few truly': 326, 'truly sympathetic': 1214, 'sympathetic roles': 1013, 'roles is': 897, 'is delight': 504, 'delight the': 250, 'the kids': 1076, 'kids are': 573, 'are as': 84, 'as grandma': 100, 'grandma says': 395, 'says more': 909, 'more like': 670, 'like dressed': 595, 'dressed up': 281, 'up midgets': 1227, 'midgets than': 661, 'than children': 1024, 'children but': 202, 'but that': 178, 'that only': 1038, 'only makes': 778, 'makes them': 625, 'them more': 1128, 'more fun': 668, 'fun to': 372, 'watch and': 1264, 'the mother': 1083, 'mother slow': 676, 'slow awakening': 961, 'awakening to': 112, 'to what': 1200, 'what happening': 1296, 'happening in': 416, 'world and': 1344, 'and under': 74, 'under her': 1221, 'her own': 439, 'own roof': 806, 'roof is': 899, 'is believable': 500, 'believable and': 141, 'and startling': 68, 'startling if': 987, 'if had': 465, 'had dozen': 411, 'dozen thumbs': 276, 'thumbs they': 1166, 'they all': 1141, 'all be': 30, 'be up': 134, 'up for': 1226, 'for this': 359, 'sure would': 1009, 'would like': 1351, 'like to': 599, 'see resurrection': 927, 'resurrection of': 890, 'of up': 749, 'up dated': 1225, 'dated seahunt': 239, 'seahunt series': 919, 'series with': 938, 'the tech': 1111, 'tech they': 1018, 'they have': 1143, 'have today': 432, 'today it': 1203, 'it would': 550, 'would bring': 1350, 'bring back': 168, 'back the': 117, 'the kid': 1075, 'kid excitement': 572, 'excitement in': 307, 'in mei': 478, 'mei grew': 655, 'grew up': 401, 'up on': 1228, 'on black': 760, 'black and': 151, 'and white': 77, 'white tv': 1313, 'tv and': 1218, 'and seahunt': 65, 'seahunt with': 920, 'with gunsmoke': 1320, 'gunsmoke were': 407, 'were my': 1294, 'my hero': 693, 'hero every': 443, 'every weekyou': 304, 'weekyou have': 1286, 'have my': 431, 'my vote': 696, 'vote for': 1251, 'for comeback': 351, 'comeback of': 213, 'of new': 738, 'new sea': 703, 'sea huntwe': 918, 'huntwe need': 463, 'need change': 699, 'change of': 196, 'of pace': 739, 'pace in': 812, 'in tv': 486, 'and this': 73, 'this would': 1161, 'would work': 1353, 'work for': 1343, 'for world': 360, 'world of': 1345, 'of under': 748, 'under water': 1223, 'water adventureoh': 1274, 'adventureoh by': 21, 'the way': 1116, 'way thank': 1278, 'thank you': 1030, 'you for': 1366, 'for an': 350, 'an outlet': 43, 'outlet like': 804, 'like this': 598, 'this to': 1159, 'to view': 1197, 'view many': 1241, 'many viewpoints': 631, 'viewpoints about': 1243, 'about tv': 13, 'the many': 1080, 'many moviesso': 629, 'moviesso any': 683, 'any ole': 80, 'ole way': 759, 'way believe': 1275, 'believe ve': 144, 've got': 1235, 'got what': 394, 'what wanna': 1299, 'wanna saywould': 1252, 'saywould be': 910, 'be nice': 129, 'nice to': 707, 'to read': 1190, 'read some': 872, 'some more': 975, 'more plus': 671, 'plus points': 842, 'points about': 845, 'about sea': 10, 'sea huntif': 917, 'huntif my': 462, 'my rhymes': 695, 'rhymes would': 892, 'would be': 1349, 'be 10': 124, '10 lines': 1, 'lines would': 601, 'would you': 1354, 'you let': 1367, 'let me': 588, 'me submit': 649, 'submit or': 1000, 'or leave': 785, 'leave me': 586, 'me out': 648, 'out to': 803, 'be in': 127, 'in doubt': 476, 'doubt and': 273, 'and have': 53, 'have me': 430, 'me to': 651, 'to quit': 1189, 'quit if': 867, 'if this': 468, 'is so': 521, 'so then': 968, 'then must': 1131, 'must go': 689, 'go so': 386, 'so lets': 965, 'lets do': 589, 'do it': 269, 'show was': 953, 'was an': 1253, 'an amazing': 40, 'amazing fresh': 39, 'fresh innovative': 366, 'innovative idea': 493, 'idea in': 464, 'the 70': 1044, '70 when': 5, 'when it': 1300, 'it first': 536, 'first aired': 341, 'aired the': 28, 'first or': 344, 'or years': 791, 'years were': 1362, 'were brilliant': 1293, 'brilliant but': 167, 'but things': 180, 'things dropped': 1148, 'dropped off': 282, 'off after': 753, 'after that': 22, 'that by': 1032, 'by 1990': 181, '1990 the': 3, 'was not': 1256, 'not really': 722, 'really funny': 879, 'funny anymore': 373, 'anymore and': 81, 'and it': 57, 'it continued': 535, 'continued its': 228, 'its decline': 553, 'decline further': 247, 'further to': 374, 'the complete': 1057, 'complete waste': 222, 'waste of': 1263, 'of time': 747, 'time it': 1170, 'is today': 525, 'today br': 1202, 'it truly': 546, 'truly disgraceful': 1211, 'disgraceful how': 267, 'how far': 458, 'far this': 322, 'show has': 948, 'has fallen': 422, 'fallen the': 317, 'the writing': 1121, 'writing is': 1358, 'is painfully': 517, 'painfully bad': 814, 'bad the': 120, 'the performances': 1092, 'performances are': 828, 'are almost': 83, 'almost as': 36, 'as bad': 97, 'bad if': 118, 'if not': 467, 'not for': 716, 'the mildly': 1081, 'mildly entertaining': 662, 'entertaining respite': 294, 'respite of': 888, 'the guest': 1073, 'guest hosts': 405, 'hosts this': 455, 'show probably': 950, 'probably wouldn': 861, 'wouldn still': 1356, 'still be': 990, 'be on': 130, 'air find': 27, 'find it': 339, 'it so': 543, 'so hard': 964, 'hard to': 418, 'to believe': 1179, 'believe that': 143, 'that the': 1042, 'same creator': 903, 'creator that': 232, 'that hand': 1034, 'hand selected': 414, 'selected the': 933, 'the original': 1088, 'original cast': 793, 'cast also': 191, 'also chose': 38, 'chose the': 203, 'the band': 1051, 'band of': 121, 'of hacks': 733, 'hacks that': 410, 'that followed': 1033, 'followed how': 349, 'how can': 457, 'can one': 186, 'one recognize': 773, 'recognize such': 880, 'such brilliance': 1003, 'brilliance and': 166, 'then see': 1132, 'see fit': 925, 'fit to': 346, 'to replace': 1191, 'replace it': 886, 'it with': 549, 'with such': 1330, 'such mediocrity': 1005, 'mediocrity felt': 653, 'felt must': 325, 'must give': 688, 'give stars': 380, 'stars out': 986, 'of respect': 741, 'respect for': 887, 'cast that': 193, 'that made': 1037, 'made this': 619, 'show such': 952, 'such huge': 1004, 'huge success': 460, 'success as': 1001, 'as it': 101, 'is now': 516, 'now the': 724, 'is just': 513, 'just awful': 563, 'awful can': 115, 'can believe': 184, 'believe it': 142, 'it still': 545, 'still on': 992, 'encouraged by': 292, 'the positive': 1096, 'positive comments': 848, 'comments about': 221, 'about this': 12, 'this film': 1152, 'film on': 333, 'on here': 762, 'here was': 442, 'was looking': 1255, 'looking forward': 611, 'forward to': 364, 'to watching': 1199, 'watching this': 1273, 'film bad': 328, 'bad mistake': 119, 'mistake ve': 663, 've seen': 1237, 'seen 950': 931, '950 films': 6, 'films and': 338, 'is truly': 526, 'truly one': 1212, 'the worst': 1120, 'worst of': 1347, 'of them': 746, 'them it': 1127, 'it awful': 532, 'awful in': 116, 'in almost': 474, 'almost every': 37, 'every way': 303, 'way editing': 1276, 'editing pacing': 287, 'pacing storyline': 813, 'storyline acting': 994, 'acting soundtrack': 16, 'soundtrack the': 980, 'film only': 334, 'only song': 779, 'song lame': 977, 'lame country': 580, 'country tune': 231, 'tune is': 1216, 'is played': 518, 'played no': 838, 'no less': 710, 'less than': 587, 'than four': 1026, 'four times': 365, 'times the': 1175, 'film looks': 332, 'looks cheap': 612, 'cheap and': 201, 'and nasty': 62, 'nasty and': 697, 'and is': 56, 'is boring': 501, 'boring in': 153, 'the extreme': 1064, 'extreme rarely': 311, 'rarely have': 869, 'have been': 426, 'been so': 138, 'so happy': 963, 'happy to': 417, 'the end': 1062, 'end credits': 293, 'credits of': 233, 'of film': 731, 'film br': 329, 'that prevents': 1039, 'prevents me': 854, 'me giving': 647, 'giving this': 383, 'this score': 1156, 'score is': 915, 'is harvey': 510, 'harvey keitel': 420, 'keitel while': 571, 'while this': 1311, 'is far': 507, 'far from': 321, 'from his': 367, 'his best': 446, 'best performance': 145, 'performance he': 827, 'he at': 434, 'at least': 107, 'least seems': 585, 'be making': 128, 'making bit': 626, 'bit of': 149, 'of an': 728, 'an effort': 41, 'effort one': 288, 'one for': 768, 'for keitel': 354, 'keitel obsessives': 570, 'obsessives only': 725, 'you like': 1368, 'like original': 597, 'original gut': 794, 'gut wrenching': 408, 'wrenching laughter': 1357, 'laughter you': 584, 'you will': 1373, 'will like': 1315, 'movie if': 679, 'you are': 1364, 'are young': 93, 'young or': 1374, 'or old': 787, 'old then': 757, 'then you': 1134, 'will love': 1316, 'love this': 615, 'movie hell': 678, 'hell even': 438, 'even my': 299, 'my mom': 694, 'mom liked': 664, 'liked it': 600, 'it br': 533, 'br great': 157, 'great camp': 397}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df_bg = pd.DataFrame(bow_rep.toarray(),columns=count_vect.get_feature_names_out())\n",
        "\n",
        "display(df_bg)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 485
        },
        "id": "x7PFRQzNt4r5",
        "outputId": "e56c6a0b-be53-479f-d360-0ffe3e26d1d3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "    10 just  10 lines  15 or  1990 the  25 years  70 when  950 films  \\\n",
              "0         0         0      0         0         0        0          0   \n",
              "1         0         0      0         0         0        0          0   \n",
              "2         0         0      0         0         0        0          0   \n",
              "3         0         0      0         0         0        0          0   \n",
              "4         1         0      0         0         0        0          0   \n",
              "5         0         0      0         0         0        0          0   \n",
              "6         0         0      1         0         1        0          0   \n",
              "7         0         1      0         0         0        0          0   \n",
              "8         0         0      0         1         0        1          0   \n",
              "9         0         0      0         0         0        0          1   \n",
              "10        0         0      0         0         0        0          0   \n",
              "\n",
              "    about human  about one  about oz  ...  you ll  you may  you must  you re  \\\n",
              "0             0          0         0  ...       0        0         0       0   \n",
              "1             0          0         1  ...       1        1         0       0   \n",
              "2             0          1         0  ...       0        0         0       0   \n",
              "3             0          0         0  ...       0        0         0       0   \n",
              "4             0          0         0  ...       0        0         1       1   \n",
              "5             1          0         0  ...       0        0         0       0   \n",
              "6             0          0         0  ...       0        0         0       0   \n",
              "7             0          0         0  ...       0        0         0       0   \n",
              "8             0          0         0  ...       0        0         0       0   \n",
              "9             0          0         0  ...       0        0         0       0   \n",
              "10            0          0         0  ...       0        0         0       0   \n",
              "\n",
              "    you will  young or  young woman  your darker  zombie br  zombie in  \n",
              "0          0         0            0            0          0          0  \n",
              "1          0         0            0            1          0          0  \n",
              "2          0         0            0            0          0          0  \n",
              "3          0         0            1            0          0          0  \n",
              "4          0         0            0            0          1          1  \n",
              "5          0         0            0            0          0          0  \n",
              "6          0         0            0            0          0          0  \n",
              "7          0         0            0            0          0          0  \n",
              "8          0         0            0            0          0          0  \n",
              "9          0         0            0            0          0          0  \n",
              "10         2         1            0            0          0          0  \n",
              "\n",
              "[11 rows x 1379 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9066ef83-da0d-44dd-affe-5455bd4a391b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>10 just</th>\n",
              "      <th>10 lines</th>\n",
              "      <th>15 or</th>\n",
              "      <th>1990 the</th>\n",
              "      <th>25 years</th>\n",
              "      <th>70 when</th>\n",
              "      <th>950 films</th>\n",
              "      <th>about human</th>\n",
              "      <th>about one</th>\n",
              "      <th>about oz</th>\n",
              "      <th>...</th>\n",
              "      <th>you ll</th>\n",
              "      <th>you may</th>\n",
              "      <th>you must</th>\n",
              "      <th>you re</th>\n",
              "      <th>you will</th>\n",
              "      <th>young or</th>\n",
              "      <th>young woman</th>\n",
              "      <th>your darker</th>\n",
              "      <th>zombie br</th>\n",
              "      <th>zombie in</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>11 rows × 1379 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9066ef83-da0d-44dd-affe-5455bd4a391b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-9066ef83-da0d-44dd-affe-5455bd4a391b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-9066ef83-da0d-44dd-affe-5455bd4a391b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Calculate the TF-IDF of these documents using scikit-learn."
      ],
      "metadata": {
        "id": "_pZmRih4QFBV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf = TfidfVectorizer()\n",
        "bow_rep_tfidf = tfidf.fit_transform(processed_docs) #note, you can use n-grams for TF-IDF as well\n",
        "\n",
        "#All words in the vocabulary.\n",
        "print(\"All words in the vocabulary\",tfidf.get_feature_names_out())\n",
        "print(\"\\n\")\n",
        "#IDF for all words in the vocabulary\n",
        "print(\"IDF for all words in the vocabulary\",tfidf.idf_)\n",
        "print(\"\\n\")\n",
        "\n",
        "\n",
        "#TFIDF representation for all documents in our corpus \n",
        "print(\"TFIDF representation for all documents in our corpus\\n\",bow_rep_tfidf.toarray()) \n",
        "print(\"\\n\")\n",
        "\n",
        "df_tfidf = pd.DataFrame(bow_rep_tfidf.toarray(),columns=tfidf.get_feature_names_out())\n",
        "\n",
        "display(df_tfidf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "hpXBkAMSCL6Y",
        "outputId": "feb3788e-917b-428d-c861-a677980df9c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All words in the vocabulary ['10' '15' '1990' '25' '70' '950' 'about' 'accustomed' 'acting' 'action'\n",
            " 'actors' 'addiction' 'adrian' 'adventureoh' 'after' 'agenda' 'agreements'\n",
            " 'air' 'aired' 'alive' 'all' 'allen' 'almost' 'also' 'amazing' 'an' 'and'\n",
            " 'another' 'anxiously' 'any' 'anymore' 'appeal' 'are' 'arguing' 'around'\n",
            " 'arthur' 'as' 'at' 'audiences' 'average' 'await' 'awakening' 'away'\n",
            " 'awful' 'back' 'bad' 'band' 'basically' 'bbc' 'be' 'become' 'been'\n",
            " 'being' 'believable' 'believe' 'best' 'bette' 'big' 'bit' 'bitches'\n",
            " 'black' 'boogeyman' 'boring' 'boy' 'br' 'bread' 'brilliance' 'brilliant'\n",
            " 'bring' 'brings' 'brutality' 'buscemi' 'but' 'by' 'called' 'camp' 'can'\n",
            " 'career' 'carol' 'case' 'cast' 'cause' 'cells' 'change' 'characters'\n",
            " 'charm' 'cheap' 'children' 'chose' 'chosen' 'christians' 'city' 'class'\n",
            " 'classic' 'closet' 'come' 'comeback' 'comedies' 'comedy' 'comes'\n",
            " 'comfortable' 'comforting' 'comments' 'complete' 'concerning'\n",
            " 'conditioned' 'connect' 'connected' 'contact' 'continued' 'control'\n",
            " 'couldn' 'country' 'creator' 'credits' 'crooked' 'crown' 'dare' 'darker'\n",
            " 'dated' 'davis' 'dawson' 'dealings' 'death' 'decade' 'decide' 'decides'\n",
            " 'decline' 'decorating' 'dedication' 'delight' 'descent' 'despite'\n",
            " 'developed' 'devil' 'dialogs' 'dialogue' 'diary' 'different' 'direction'\n",
            " 'director' 'disappears' 'disappointed' 'discerns' 'discomforting'\n",
            " 'disgraceful' 'divorcing' 'do' 'dodgy' 'doesn' 'done' 'doubt' 'down'\n",
            " 'dozen' 'drama' 'dream' 'dressed' 'dropped' 'drugs' 'due' 'each'\n",
            " 'editing' 'effort' 'em' 'emerald' 'encounter' 'encouraged' 'end'\n",
            " 'entertaining' 'entire' 'entries' 'episode' 'even' 'ever' 'every'\n",
            " 'exactly' 'excitement' 'expected' 'experience' 'experimental' 'extreme'\n",
            " 'extremely' 'eyes' 'face' 'fact' 'faint' 'fallen' 'family' 'fantasy'\n",
            " 'far' 'fashion' 'favorite' 'felt' 'few' 'fighting' 'film' 'filming'\n",
            " 'films' 'find' 'first' 'fit' 'flat' 'focuses' 'followed' 'for' 'forget'\n",
            " 'forward' 'four' 'fresh' 'friends' 'from' 'fronts' 'fulfillment' 'fully'\n",
            " 'fun' 'funny' 'further' 'gangstas' 'get' 'gets' 'give' 'given' 'gives'\n",
            " 'giving' 'glass' 'go' 'goes' 'going' 'good' 'got' 'grandma' 'graphic'\n",
            " 'great' 'grenier' 'grew' 'grown' 'guard' 'guards' 'guest' 'guided'\n",
            " 'gunsmoke' 'gut' 'habitat' 'hacks' 'had' 'halliwell' 'hand' 'happened'\n",
            " 'happening' 'happy' 'hard' 'hardcore' 'harvey' 'has' 'have' 'having' 'he'\n",
            " 'hearted' 'hell' 'her' 'here' 'hero' 'high' 'his' 'home' 'hooked' 'hosts'\n",
            " 'hot' 'how' 'huge' 'human' 'huntif' 'huntwe' 'idea' 'if' 'ignore' 'image'\n",
            " 'imperioli' 'impressed' 'in' 'inhabits' 'injustice' 'inmates'\n",
            " 'innovative' 'instead' 'interesting' 'into' 'inwards' 'irish' 'is' 'it'\n",
            " 'italians' 'its' 'jake' 'jewel' 'johanson' 'jumped' 'just' 'kane'\n",
            " 'keitel' 'kid' 'kids' 'kill' 'killer' 'know' 'knowledge' 'lack' 'lame'\n",
            " 'last' 'latinos' 'laughed' 'laughter' 'least' 'leave' 'less' 'let' 'lets'\n",
            " 'levels' 'life' 'light' 'likable' 'like' 'liked' 'lines' 'little' 'live'\n",
            " 'll' 'loneliness' 'look' 'looking' 'looks' 'love' 'luck' 'lukas'\n",
            " 'luxurious' 'made' 'main' 'mainly' 'mainstream' 'make' 'makes' 'making'\n",
            " 'managed' 'mannered' 'many' 'manyaryans' 'master' 'masterful' 'match'\n",
            " 'mattei' 'maximum' 'may' 'me' 'meaningless' 'mediocrity' 'meet' 'mei'\n",
            " 'mentioned' 'mess' 'michael' 'middle' 'midgets' 'mildly' 'mistake' 'mom'\n",
            " 'money' 'more' 'moreso' 'most' 'mother' 'movie' 'moviesso' 'mr' 'murals'\n",
            " 'muslims' 'must' 'my' 'nasty' 'need' 'never' 'new' 'next' 'nice' 'nickel'\n",
            " 'nickname' 'no' 'noble' 'not' 'now' 'obsessives' 'of' 'off' 'offers' 'ok'\n",
            " 'old' 'ole' 'on' 'one' 'only' 'opera' 'or' 'order' 'original' 'orton'\n",
            " 'oswald' 'other' 'our' 'out' 'outlet' 'own' 'oz' 'pace' 'pacing'\n",
            " 'painfully' 'painted' 'parents' 'particularly' 'pat' 'paul' 'penitentary'\n",
            " 'people' 'performance' 'performances' 'performed' 'person' 'petter'\n",
            " 'picture' 'pictures' 'piece' 'place' 'play' 'played' 'playing' 'plays'\n",
            " 'plot' 'plus' 'point' 'points' 'polari' 'portrait' 'positive' 'power'\n",
            " 'prada' 'preachy' 'present' 'pretty' 'prevents' 'previous' 'prison'\n",
            " 'privacy' 'probably' 'production' 'proof' 'pulls' 'punches' 'quit'\n",
            " 'rambo' 'rarely' 'rather' 're' 'read' 'ready' 'real' 'realism' 'realize'\n",
            " 'really' 'recognize' 'references' 'regards' 'relations' 'remains'\n",
            " 'replace' 'respect' 'respite' 'rest' 'resurrection' 'review' 'reviewers'\n",
            " 'rhymes' 'right' 'risk' 'roles' 'romanceoz' 'roof' 'rosario' 'ruins'\n",
            " 'sacrifice' 'same' 'saw' 'say' 'says' 'saywould' 'scarlet' 'scenes'\n",
            " 'schnitzler' 'score' 'scuffles' 'sea' 'seahunt' 'seamless' 'section'\n",
            " 'security' 'see' 'seems' 'seen' 'selected' 'selflessness' 'sense'\n",
            " 'senses' 'serial' 'series' 'set' 'sets' 'sex' 'sexy' 'shady' 'she'\n",
            " 'sheen' 'shots' 'show' 'shows' 'side' 'similar' 'simplistic' 'sincere'\n",
            " 'sitting' 'situations' 'skills' 'slow' 'slower' 'so' 'soap' 'sold'\n",
            " 'solid' 'some' 'sometimes' 'song' 'sophisticated' 'souls' 'soundtrack'\n",
            " 'spend' 'spirited' 'spots' 'stages' 'stares' 'stars' 'startling' 'state'\n",
            " 'steve' 'still' 'story' 'storyline' 'street' 'struck' 'stunning' 'style'\n",
            " 'stylishly' 'submit' 'success' 'such' 'suddenly' 'summer' 'superman'\n",
            " 'sure' 'surface' 'surreal' 'suspected' 'sympathetic' 'taken' 'talented'\n",
            " 'taste' 'tears' 'tech' 'technique' 'techniques' 'telling' 'terribly'\n",
            " 'terrificly' 'than' 'thank' 'that' 'the' 'theater' 'their' 'them' 'theme'\n",
            " 'then' 'there' 'these' 'they' 'thing' 'things' 'thinks' 'this' 'thought'\n",
            " 'thriller' 'thumbs' 'time' 'times' 'timid' 'to' 'today' 'tone' 'too'\n",
            " 'totally' 'touch' 'traditional' 'transfers' 'truly' 'trust' 'tune'\n",
            " 'turned' 'tv' 'unassuming' 'uncomfortable' 'under' 'unflinching' 'up'\n",
            " 'us' 'use' 'variation' 've' 'very' 'view' 'viewingthats' 'viewpoints'\n",
            " 'violence' 'visually' 'vivid' 'voices' 'vote' 'wanna' 'was' 'waste'\n",
            " 'watch' 'watchable' 'watched' 'watching' 'water' 'way' 'we' 'wears'\n",
            " 'weekend' 'weekyou' 'well' 'were' 'what' 'when' 'where' 'which' 'while'\n",
            " 'white' 'who' 'will' 'williams' 'wish' 'with' 'wittier' 'witty' 'woman'\n",
            " 'wonderful' 'woody' 'word' 'work' 'world' 'worst' 'worth' 'would'\n",
            " 'wouldn' 'wrenching' 'writing' 'written' 'years' 'york' 'you' 'young'\n",
            " 'your' 'zombie']\n",
            "\n",
            "\n",
            "IDF for all words in the vocabulary [2.38629436 2.79175947 2.79175947 2.79175947 2.79175947 2.79175947\n",
            " 1.69314718 2.79175947 2.38629436 2.79175947 2.79175947 2.79175947\n",
            " 2.79175947 2.79175947 2.38629436 2.79175947 2.79175947 2.38629436\n",
            " 2.79175947 2.79175947 1.69314718 2.79175947 2.38629436 2.79175947\n",
            " 2.79175947 1.87546874 1.18232156 2.79175947 2.79175947 2.79175947\n",
            " 2.79175947 2.79175947 1.28768207 2.79175947 2.79175947 2.79175947\n",
            " 1.69314718 2.38629436 2.79175947 2.79175947 2.79175947 2.79175947\n",
            " 2.79175947 2.38629436 2.79175947 2.38629436 2.79175947 2.79175947\n",
            " 2.79175947 1.40546511 2.38629436 2.38629436 2.38629436 2.79175947\n",
            " 2.38629436 2.38629436 2.79175947 2.79175947 2.79175947 2.79175947\n",
            " 2.79175947 2.79175947 2.38629436 2.79175947 1.28768207 2.79175947\n",
            " 2.79175947 2.79175947 2.79175947 2.79175947 2.79175947 2.79175947\n",
            " 1.5389965  1.87546874 2.79175947 2.79175947 2.09861229 2.79175947\n",
            " 2.79175947 2.79175947 2.38629436 2.79175947 2.79175947 2.79175947\n",
            " 2.38629436 2.79175947 2.79175947 2.79175947 2.79175947 2.79175947\n",
            " 2.79175947 2.38629436 2.79175947 2.79175947 2.79175947 2.79175947\n",
            " 2.79175947 2.79175947 2.38629436 2.79175947 2.79175947 2.79175947\n",
            " 2.79175947 2.79175947 2.79175947 2.79175947 2.79175947 2.79175947\n",
            " 2.79175947 2.79175947 2.79175947 2.79175947 2.79175947 2.79175947\n",
            " 2.79175947 2.79175947 2.79175947 2.38629436 2.79175947 2.79175947\n",
            " 2.79175947 2.79175947 2.79175947 2.79175947 2.79175947 2.79175947\n",
            " 2.79175947 2.79175947 2.79175947 2.79175947 2.79175947 2.79175947\n",
            " 2.79175947 2.79175947 2.79175947 2.79175947 2.79175947 2.79175947\n",
            " 2.79175947 2.79175947 2.79175947 2.79175947 2.79175947 2.79175947\n",
            " 2.79175947 2.79175947 2.79175947 2.38629436 2.79175947 2.79175947\n",
            " 2.79175947 2.79175947 2.38629436 2.79175947 2.79175947 2.79175947\n",
            " 2.79175947 2.79175947 2.79175947 2.79175947 2.79175947 2.38629436\n",
            " 2.79175947 2.79175947 2.79175947 2.79175947 2.79175947 2.79175947\n",
            " 2.79175947 2.79175947 2.79175947 2.79175947 2.38629436 2.79175947\n",
            " 2.09861229 2.38629436 2.79175947 2.79175947 2.79175947 2.79175947\n",
            " 2.79175947 2.79175947 2.79175947 2.79175947 2.79175947 2.79175947\n",
            " 2.79175947 2.79175947 2.79175947 2.09861229 2.79175947 2.79175947\n",
            " 2.79175947 2.79175947 2.79175947 2.09861229 2.79175947 2.79175947\n",
            " 2.38629436 2.09861229 2.79175947 2.79175947 2.79175947 2.79175947\n",
            " 1.40546511 2.79175947 2.79175947 2.79175947 2.79175947 2.79175947\n",
            " 2.38629436 2.79175947 2.79175947 2.79175947 2.79175947 2.79175947\n",
            " 2.79175947 2.79175947 2.79175947 2.38629436 2.79175947 2.79175947\n",
            " 2.79175947 2.79175947 2.79175947 2.09861229 2.79175947 2.79175947\n",
            " 2.79175947 2.09861229 2.79175947 2.79175947 2.09861229 2.79175947\n",
            " 2.79175947 2.79175947 2.79175947 2.79175947 2.79175947 2.79175947\n",
            " 2.79175947 2.79175947 2.79175947 2.79175947 2.79175947 2.79175947\n",
            " 2.79175947 2.79175947 2.79175947 2.79175947 2.79175947 2.79175947\n",
            " 2.79175947 1.87546874 1.69314718 2.79175947 2.38629436 2.38629436\n",
            " 2.79175947 2.38629436 2.79175947 2.79175947 2.79175947 1.69314718\n",
            " 2.38629436 2.79175947 2.79175947 2.79175947 2.38629436 2.79175947\n",
            " 2.79175947 2.79175947 2.79175947 2.79175947 1.5389965  2.79175947\n",
            " 2.79175947 2.79175947 2.79175947 1.28768207 2.79175947 2.79175947\n",
            " 2.79175947 2.79175947 2.79175947 2.79175947 2.38629436 2.79175947\n",
            " 2.79175947 1.18232156 1.28768207 2.79175947 2.09861229 2.79175947\n",
            " 2.79175947 2.79175947 2.79175947 1.87546874 2.79175947 2.79175947\n",
            " 2.79175947 2.79175947 2.38629436 2.79175947 2.79175947 2.79175947\n",
            " 2.79175947 2.79175947 2.79175947 2.79175947 2.79175947 2.79175947\n",
            " 2.79175947 2.79175947 2.79175947 2.79175947 2.79175947 2.79175947\n",
            " 2.38629436 2.79175947 2.79175947 1.87546874 2.79175947 2.79175947\n",
            " 2.38629436 2.79175947 2.79175947 2.79175947 2.79175947 2.79175947\n",
            " 2.79175947 2.09861229 2.79175947 2.79175947 2.79175947 2.79175947\n",
            " 2.79175947 2.79175947 2.79175947 2.38629436 2.79175947 2.79175947\n",
            " 2.79175947 2.79175947 2.38629436 2.79175947 2.79175947 2.79175947\n",
            " 2.79175947 2.79175947 2.79175947 2.38629436 2.09861229 2.79175947\n",
            " 2.79175947 2.79175947 2.79175947 2.79175947 2.79175947 2.38629436\n",
            " 2.79175947 2.79175947 2.79175947 2.79175947 2.79175947 2.79175947\n",
            " 1.87546874 2.79175947 2.38629436 2.79175947 1.87546874 2.79175947\n",
            " 2.79175947 2.79175947 2.79175947 2.09861229 2.09861229 2.38629436\n",
            " 2.79175947 2.09861229 2.38629436 2.79175947 2.79175947 2.79175947\n",
            " 2.79175947 2.09861229 2.79175947 1.5389965  2.79175947 2.79175947\n",
            " 1.18232156 2.79175947 2.79175947 2.79175947 2.09861229 2.79175947\n",
            " 1.40546511 1.40546511 1.87546874 2.79175947 1.40546511 2.79175947\n",
            " 2.38629436 2.79175947 2.79175947 2.79175947 2.79175947 1.69314718\n",
            " 2.79175947 2.38629436 2.79175947 2.79175947 2.79175947 2.79175947\n",
            " 2.79175947 2.79175947 2.79175947 2.79175947 2.79175947 2.79175947\n",
            " 2.79175947 2.38629436 2.79175947 2.79175947 2.79175947 2.79175947\n",
            " 2.79175947 2.79175947 2.79175947 2.79175947 2.79175947 2.79175947\n",
            " 2.79175947 2.79175947 2.79175947 2.79175947 2.38629436 2.79175947\n",
            " 2.79175947 2.79175947 2.79175947 2.79175947 2.79175947 2.79175947\n",
            " 2.79175947 2.79175947 2.79175947 2.79175947 2.79175947 2.79175947\n",
            " 2.38629436 2.79175947 2.79175947 2.79175947 2.79175947 2.79175947\n",
            " 2.79175947 2.79175947 2.79175947 2.79175947 2.79175947 2.79175947\n",
            " 2.79175947 2.79175947 2.79175947 2.38629436 2.79175947 2.79175947\n",
            " 2.79175947 2.79175947 2.79175947 2.79175947 2.79175947 2.79175947\n",
            " 2.79175947 2.79175947 2.79175947 2.79175947 2.79175947 2.38629436\n",
            " 2.79175947 2.79175947 2.79175947 2.79175947 2.79175947 2.79175947\n",
            " 2.79175947 2.38629436 2.79175947 2.38629436 2.79175947 2.79175947\n",
            " 2.79175947 2.38629436 2.79175947 2.79175947 2.79175947 2.79175947\n",
            " 2.79175947 2.79175947 2.79175947 2.79175947 1.40546511 2.38629436\n",
            " 2.38629436 2.79175947 2.79175947 2.79175947 2.79175947 2.79175947\n",
            " 2.79175947 2.79175947 2.79175947 2.79175947 2.79175947 2.79175947\n",
            " 2.79175947 2.79175947 2.79175947 2.38629436 2.79175947 2.79175947\n",
            " 2.79175947 2.79175947 2.79175947 2.79175947 2.79175947 2.79175947\n",
            " 2.79175947 2.79175947 1.87546874 2.79175947 2.79175947 2.79175947\n",
            " 1.87546874 2.79175947 2.79175947 2.79175947 2.79175947 2.79175947\n",
            " 2.79175947 2.79175947 2.79175947 2.79175947 2.79175947 2.79175947\n",
            " 2.79175947 2.79175947 2.79175947 2.38629436 2.79175947 2.79175947\n",
            " 2.79175947 2.79175947 2.79175947 2.79175947 2.79175947 2.79175947\n",
            " 2.38629436 2.79175947 2.79175947 2.79175947 2.79175947 2.79175947\n",
            " 2.79175947 2.79175947 2.79175947 2.79175947 2.79175947 2.79175947\n",
            " 2.79175947 2.79175947 2.79175947 2.79175947 2.79175947 2.79175947\n",
            " 2.79175947 2.79175947 1.69314718 2.79175947 1.5389965  1.18232156\n",
            " 2.79175947 2.09861229 2.09861229 2.79175947 1.69314718 2.79175947\n",
            " 2.79175947 1.69314718 2.09861229 2.38629436 2.79175947 1.18232156\n",
            " 2.79175947 2.79175947 2.79175947 1.5389965  2.38629436 2.79175947\n",
            " 1.18232156 2.38629436 2.79175947 2.38629436 2.79175947 2.79175947\n",
            " 2.79175947 2.79175947 1.87546874 2.79175947 2.79175947 2.79175947\n",
            " 2.79175947 2.79175947 2.79175947 2.09861229 2.79175947 2.38629436\n",
            " 2.38629436 2.38629436 2.79175947 2.09861229 2.38629436 2.79175947\n",
            " 2.79175947 2.79175947 2.79175947 2.79175947 2.79175947 2.79175947\n",
            " 2.79175947 2.79175947 1.87546874 2.79175947 2.38629436 2.79175947\n",
            " 2.38629436 1.87546874 2.79175947 1.87546874 2.38629436 2.79175947\n",
            " 2.79175947 2.79175947 1.87546874 2.38629436 1.87546874 2.09861229\n",
            " 2.09861229 1.87546874 2.38629436 2.79175947 2.79175947 2.79175947\n",
            " 2.79175947 2.79175947 1.40546511 2.79175947 2.79175947 2.79175947\n",
            " 2.38629436 2.79175947 2.79175947 2.38629436 2.09861229 2.79175947\n",
            " 2.79175947 2.38629436 2.38629436 2.79175947 2.79175947 2.79175947\n",
            " 2.09861229 2.79175947 1.69314718 2.38629436 2.79175947 2.79175947]\n",
            "\n",
            "\n",
            "TFIDF representation for all documents in our corpus\n",
            " [[0.         0.         0.         ... 0.         0.         0.        ]\n",
            " [0.         0.         0.         ... 0.         0.04937801 0.        ]\n",
            " [0.         0.         0.         ... 0.         0.         0.        ]\n",
            " ...\n",
            " [0.         0.         0.06970709 ... 0.         0.         0.        ]\n",
            " [0.         0.         0.         ... 0.         0.         0.        ]\n",
            " [0.         0.         0.         ... 0.1593356  0.         0.        ]]\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "          10       15      1990       25        70      950     about  \\\n",
              "0   0.000000  0.00000  0.000000  0.00000  0.000000  0.00000  0.000000   \n",
              "1   0.000000  0.00000  0.000000  0.00000  0.000000  0.00000  0.029947   \n",
              "2   0.000000  0.00000  0.000000  0.00000  0.000000  0.00000  0.044560   \n",
              "3   0.000000  0.00000  0.000000  0.00000  0.000000  0.00000  0.000000   \n",
              "4   0.072624  0.00000  0.000000  0.00000  0.000000  0.00000  0.000000   \n",
              "5   0.000000  0.00000  0.000000  0.00000  0.000000  0.00000  0.066109   \n",
              "6   0.000000  0.10307  0.000000  0.10307  0.000000  0.00000  0.000000   \n",
              "7   0.070431  0.00000  0.000000  0.00000  0.000000  0.00000  0.099946   \n",
              "8   0.000000  0.00000  0.069707  0.00000  0.069707  0.00000  0.000000   \n",
              "9   0.000000  0.00000  0.000000  0.00000  0.000000  0.09654  0.058550   \n",
              "10  0.000000  0.00000  0.000000  0.00000  0.000000  0.00000  0.000000   \n",
              "\n",
              "    accustomed    acting    action  ...    wouldn  wrenching   writing  \\\n",
              "0     0.000000  0.000000  0.000000  ...  0.000000   0.000000  0.000000   \n",
              "1     0.049378  0.000000  0.000000  ...  0.042207   0.000000  0.000000   \n",
              "2     0.000000  0.000000  0.000000  ...  0.000000   0.000000  0.000000   \n",
              "3     0.000000  0.000000  0.000000  ...  0.000000   0.000000  0.000000   \n",
              "4     0.000000  0.000000  0.000000  ...  0.000000   0.000000  0.000000   \n",
              "5     0.000000  0.046587  0.054502  ...  0.000000   0.000000  0.000000   \n",
              "6     0.000000  0.000000  0.000000  ...  0.000000   0.000000  0.000000   \n",
              "7     0.000000  0.000000  0.000000  ...  0.000000   0.000000  0.000000   \n",
              "8     0.000000  0.000000  0.000000  ...  0.059583   0.000000  0.069707   \n",
              "9     0.000000  0.082519  0.000000  ...  0.000000   0.000000  0.000000   \n",
              "10    0.000000  0.000000  0.000000  ...  0.000000   0.186409  0.000000   \n",
              "\n",
              "     written     years      york       you     young      your    zombie  \n",
              "0   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
              "1   0.000000  0.000000  0.000000  0.089840  0.000000  0.049378  0.000000  \n",
              "2   0.073474  0.000000  0.000000  0.044560  0.000000  0.000000  0.000000  \n",
              "3   0.000000  0.062851  0.000000  0.000000  0.071467  0.000000  0.000000  \n",
              "4   0.000000  0.000000  0.000000  0.103057  0.000000  0.000000  0.169927  \n",
              "5   0.000000  0.000000  0.054502  0.000000  0.000000  0.000000  0.000000  \n",
              "6   0.000000  0.077480  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
              "7   0.000000  0.000000  0.000000  0.099946  0.000000  0.000000  0.000000  \n",
              "8   0.000000  0.052400  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
              "9   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
              "10  0.000000  0.000000  0.000000  0.452213  0.159336  0.000000  0.000000  \n",
              "\n",
              "[11 rows x 678 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5ce18794-c914-4c16-9fa0-0a3e3a3d586d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>10</th>\n",
              "      <th>15</th>\n",
              "      <th>1990</th>\n",
              "      <th>25</th>\n",
              "      <th>70</th>\n",
              "      <th>950</th>\n",
              "      <th>about</th>\n",
              "      <th>accustomed</th>\n",
              "      <th>acting</th>\n",
              "      <th>action</th>\n",
              "      <th>...</th>\n",
              "      <th>wouldn</th>\n",
              "      <th>wrenching</th>\n",
              "      <th>writing</th>\n",
              "      <th>written</th>\n",
              "      <th>years</th>\n",
              "      <th>york</th>\n",
              "      <th>you</th>\n",
              "      <th>young</th>\n",
              "      <th>your</th>\n",
              "      <th>zombie</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.029947</td>\n",
              "      <td>0.049378</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.042207</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.089840</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.049378</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.044560</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.073474</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.044560</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.062851</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.071467</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.072624</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.103057</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.169927</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.066109</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.046587</td>\n",
              "      <td>0.054502</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.054502</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.10307</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.10307</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.077480</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.070431</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.099946</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.099946</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.069707</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.069707</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.059583</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.069707</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.052400</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.09654</td>\n",
              "      <td>0.058550</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.082519</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.186409</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.452213</td>\n",
              "      <td>0.159336</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>11 rows × 678 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5ce18794-c914-4c16-9fa0-0a3e3a3d586d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-5ce18794-c914-4c16-9fa0-0a3e3a3d586d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-5ce18794-c914-4c16-9fa0-0a3e3a3d586d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Describe 2 reasons why one hot encoding is rarely used for text representation. Are\n",
        "there other vector space model representations that also share the issues you described?\n",
        "Name them."
      ],
      "metadata": {
        "id": "JldrwgmKQHCg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "According to the lectures where we discussed the drawbacks of one hot encoding, there are several drawbacks for using one hot encoding. The first one is that the **relationships between tokens lost** because  One hot encoding does not capture any information about the relationships between words. It represents each word as a unique entity, ignoring the similarity or dissimilarity between words. For instance, in the exercise that we had, one-hot encoding, the words \"kill\" and \"killer\" are represented as completely different entities even though they have semantic similarities. This can result in a loss of semantic information, making it harder for machine learning models to capture the meaning of the text and make accurate predictions. The second reson can be **sparcity** because one hot encoding results in a high-dimensional sparse matrix, which means that most of the entries in the matrix are zero. This can result in a large amount of memory usage and slow down the computation. "
      ],
      "metadata": {
        "id": "_fDkvy0SDjlu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the second part, another vector space model representation that has the same issue is **bag of words** where the size of the vector still increases with the size of the vocabulary so it has the sparsity issue. \n",
        "There is another limtation for using one hot encoding and bag of words which is OOV (Out of Vocabulary) which refers to the words that are not present in the vocabulary used for encoding the text. One hot encoding requires a fixed vocabulary of unique words to be defined beforehand, and any word that is not present in this vocabulary will be treated as an out of vocabulary word or unknown word. This problem can also be seen in Bag of words."
      ],
      "metadata": {
        "id": "EUsCmgnHIXML"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.quora.com/What-are-the-advantages-and-disadvantages-of-TF-IDF"
      ],
      "metadata": {
        "id": "q0Y2nhd0LoVV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " **TF-IDF** can help identify important words in a document, but it also does not capture the semantic meaning of the words. For example, the words \"car\" and \"automobile\" may have similar importance in a document, but TF-IDF does not capture the fact that they have the same or similar meanings. So overal, One hot encoding, bag of words, and TF-IDF all have the problem with semantic similarities. "
      ],
      "metadata": {
        "id": "Q4KQ-B4NLJbY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 3"
      ],
      "metadata": {
        "id": "fBUgbzSTP5nX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Assume you have to make your own training set to pass as input to Word2Vec instead of\n",
        "just a list of lists of text. Write a windowing function that can take text as input and make 1\n",
        "training set in the CBOW format as seen in slide 5 of the Word2Vec slides. Make the window\n",
        "size a parameter of your function that can be changed. Test your code with file nlptext.txt.\n",
        "Note that you do not have to train a Word2Vec model with this dataset.*italicized text*"
      ],
      "metadata": {
        "id": "cLAHatwuQSxK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import numpy as np\n",
        "\n",
        "# def create_cbow_training_set(file_path, window_size):\n",
        "#     # Load the text file\n",
        "#     with open(file_path, 'r') as f:\n",
        "#         text = f.read()\n",
        "    \n",
        "#     # Split the text into a list of words\n",
        "#     words = text.split()\n",
        "    \n",
        "#     # Create a list of unique words and a dictionary to map each word to an index\n",
        "#     vocab = list(set(words))\n",
        "#     word_to_idx = {word: i for i, word in enumerate(vocab)}\n",
        "    \n",
        "#     # Initialize the input and output arrays for the training set\n",
        "#     input_data = []\n",
        "#     output_data = []\n",
        "    \n",
        "#     # Loop over all words in the text\n",
        "#     for i, word in enumerate(words):\n",
        "#         # Skip the first `window_size` and last `window_size` words\n",
        "#         if i < window_size or i >= len(words) - window_size:\n",
        "#             continue\n",
        "        \n",
        "#         # Get the context words for the current word\n",
        "#         context_words = words[i - window_size:i] + words[i + 1:i + window_size + 1]\n",
        "        \n",
        "#         # Convert the context words to indices\n",
        "#         context_idxs = [word_to_idx[w] for w in context_words]\n",
        "        \n",
        "#         # Add the context words to the input data\n",
        "#         input_data.append(context_idxs)\n",
        "        \n",
        "#         # Add the current word to the output data\n",
        "#         output_data.append(word_to_idx[word])\n",
        "    \n",
        "#     input_data = np.array(input_data)\n",
        "#     output_data = np.array(output_data)\n",
        "    \n",
        "#     return input_data, output_data\n"
      ],
      "metadata": {
        "id": "Ohwt-B3LsddC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_training_set(text, window_size=5):\n",
        "    # Split the text into individual words\n",
        "    words = text.split()\n",
        "\n",
        "    # Create a list to hold the training data\n",
        "    training_set = []\n",
        "\n",
        "    # Loop over each word in the text\n",
        "    for i, word in enumerate(words):\n",
        "        # Define the context window\n",
        "        start_index = max(0, i - window_size)\n",
        "        end_index = min(len(words) - 1, i + window_size)\n",
        "        context = words[start_index:i] + words[i+1:end_index+1]\n",
        "\n",
        "        # Add the current word and its context to the training set\n",
        "        training_set.append((word, context))\n",
        "\n",
        "    return training_set\n"
      ],
      "metadata": {
        "id": "SC7iIlWP2-g8"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The text parameter is the input text to create a training set for, and the window_size parameter is the size of the context window to use for each word.\n",
        "For each word, the function defines a context window consisting of the window_size words to the left and right of the current word (unless the start or end of the text is reached). The function then adds a tuple consisting of the current word and its context to the training set."
      ],
      "metadata": {
        "id": "3SEJzyN0tN2v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir ex3"
      ],
      "metadata": {
        "id": "wbBtU6PttxW9"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read in the contents of the file\n",
        "with open('/content/ex3/nlptext.txt', 'r') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# Generate the training set\n",
        "training_set = create_training_set(text, window_size=2)\n",
        "\n",
        "# Print the first five examples in the training set\n",
        "print(training_set[:5])\n",
        "\n"
      ],
      "metadata": {
        "id": "yJgl-s-Ks53B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "699cc08b-5092-4ed2-f991-281ed1b90edf"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Most', ['natural', 'language']), ('natural', ['Most', 'language', 'processing']), ('language', ['Most', 'natural', 'processing', 'systems']), ('processing', ['natural', 'language', 'systems', 'were']), ('systems', ['language', 'processing', 'were', 'based'])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This output shows that the training set is generated as expected, with each example consisting of a target word and its context words. Note that this function does not actually train a Word2Vec model; it just generates the training set in the format expected by Word2Vec."
      ],
      "metadata": {
        "id": "GlRshucO38Sx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Suppose you are training a Word2Vec model with the text in part A. Given that you set\n",
        "your hyperparameters for Word2Vec as size=50, window=3, and min_count=5, what will the\n",
        "dimensions of the embedding matrix be and why (i.e. what do the dimensions of the embedding\n",
        "matrix represent)? What will the dimensions be for the context matrix? Assume you do not do\n",
        "any preprocessing of the text other than simple word tokenization with NLTK."
      ],
      "metadata": {
        "id": "RQHPxYrAQXpQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we train a Word2Vec model on the given text with hyperparameters size=50, window=3, and min_count=5, the dimensions of the embedding matrix will be V x 50, where V is the size of the vocabulary in the corpus. \n",
        "The embedding matrix contains the learned representations for each word in the vocabulary. Each row of the matrix corresponds to the embedding vector for a particular word in the vocabulary. Since we set the embedding dimension to be 50, each embedding vector will have 50 dimensions. The number of rows in the embedding matrix is equal to the size of the vocabulary, which is the number of unique words in the corpus that have a frequency greater than or equal to min_count.\n",
        "\n",
        "The context matrix, on the other hand, is not directly output by the Word2Vec model. Instead, the context matrix is used to learn the embedding matrix during the training process. The context matrix is an intermediate representation that stores the one-hot encoded context vectors for each training example. The context matrix has dimensions N x V, where N is the total number of training examples and V is the size of the vocabulary.\n",
        "\n",
        "During training, the model uses the context matrix to update the weights in the embedding matrix to maximize the likelihood of predicting the context words given a target word. The output of the Word2Vec model is the learned embedding matrix, not the context matrix.\n",
        "\n",
        "It's worth noting that the actual size of the embedding matrix and vocabulary will depend on the specific text being used and the frequency of the words in the corpus. However, given the hyperparameters specified and assuming no preprocessing other than simple tokenization, the embedding matrix will have dimensions V x 50."
      ],
      "metadata": {
        "id": "S8i-uXHg1LJZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. What is the purpose of negative sampling when training Word2Vec?"
      ],
      "metadata": {
        "id": "B_9Hs6ekQbd1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://analyticsindiamag.com/how-to-use-negative-sampling-with-word2vec-model/#:~:text=When%20the%20size%20of%20training,to%20get%20modified%20during%20training."
      ],
      "metadata": {
        "id": "i-VVg01g1zNf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The purpose of negative sampling in Word2Vec is to improve the efficiency and training time of the model, while still maintaining the quality of the learned word embeddings.\n",
        "\n",
        "In the original Word2Vec algorithm, the training objective is to maximize the probability of predicting the surrounding context words given a target word (CBOW model) or the target word given its surrounding context words (skip-gram model). To achieve this, the model needs to compute the probability of selecting each word in the vocabulary as a context word (or target word) for a given target word (or context word), which involves computing the softmax function over the entire vocabulary.\n",
        "\n",
        "However, computing the softmax function can be computationally expensive and slow, especially when dealing with large vocabularies. To address this issue, negative sampling was introduced in the Word2Vec algorithm.\n",
        "\n",
        "Negative sampling involves randomly selecting a small number of \"negative\" examples (i.e., words that are not in the current context of the target word) and optimizing the model to predict these negative examples with low probability. Specifically, instead of computing the softmax over the entire vocabulary, the model only needs to compute the probability of selecting a small number of negative examples and the true context word. By doing this, the model can be trained much faster and more efficiently.\n",
        "\n",
        "In practice, negative sampling involves randomly selecting a small number (e.g., 5-20) of negative examples for each positive example during training. The number of negative examples is usually much smaller than the size of the entire vocabulary, which can be on the order of millions of words. The actual number of negative examples to use is a hyperparameter that can be tuned to balance training time and model performance."
      ],
      "metadata": {
        "id": "uBt9vgSP13cZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Why does CBOW train faster than Skip-gram? Hint: Think about how each model is\n",
        "updated."
      ],
      "metadata": {
        "id": "EM8u5GRkQeup"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://stats.stackexchange.com/questions/321304/why-does-the-skipgram-model-takes-more-time-to-train-compared-to-cbow"
      ],
      "metadata": {
        "id": "HI3Pp3o11rZL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "CBOW generally trains faster than Skip-gram because CBOW updates the word embeddings for all the words in a given context in one go, while Skip-gram updates the word embeddings for each context word individually.\n",
        "\n",
        "In the CBOW model, the input to the neural network is a bag of context words, and the output is the predicted target word. The weights connecting the input layer to the hidden layer represent the embeddings for each of the context words, and these weights are updated based on the error in predicting the target word. Since all the context words share the same weights, updating the weights for each context word is equivalent to updating the weights for all the context words at once. This can make the training process more efficient.\n",
        "\n",
        "In contrast, the Skip-gram model predicts the context words given a target word. To do this, the model needs to update the weights for each context word independently, since each context word has its own embedding in the model. This means that for each training example, the model needs to update the weights for each context word, which can be slower and more computationally expensive than updating the weights for all the context words at once in CBOW.\n",
        "\n",
        "However, it's worth noting that the efficiency of training depends on the specific implementation and the size of the dataset being used. In some cases, Skip-gram may be faster than CBOW, especially when the training data is very large or the context window size is very small."
      ],
      "metadata": {
        "id": "NETmwcPr3C65"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 4"
      ],
      "metadata": {
        "id": "lznVisQmQksp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "SpaCy uses a modified version of Word2Vec to get token representations called FastText.\n",
        "FastText is essentially the same as Word2Vec, except that instead of operating on tokens of\n",
        "entire words, it operates on sets of characters.\n"
      ],
      "metadata": {
        "id": "SC_2O7CUQoJ8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Import fasttext from Gensim and get the representation of at least two out of vocab words\n",
        "when you train a model with corpus = [[\"horse\", \"pulled\", \"cart\"], [\"dog\", \"say\", \"woof\"]] and\n",
        "the most similar words to it. Will any word work with this fasttext model? Why or why not?"
      ],
      "metadata": {
        "id": "gWDkKoCaQtMu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models.fasttext import FastText\n",
        "\n",
        "# Define the corpus\n",
        "corpus = [[\"horse\", \"pulled\", \"cart\"], [\"dog\", \"say\", \"woof\"]]\n",
        "\n",
        "# Train the FastText model\n",
        "model = FastText(corpus, size=100, window=5, min_count=1, workers=4, sg=1)\n",
        "\n",
        "# Get the representations and most similar words for two out-of-vocabulary words\n",
        "word1 = \"push\"\n",
        "word2 = \"cat\"\n",
        "vector1 = model.wv[word1]\n",
        "vector2 = model.wv[word2]\n",
        "similars1 = model.wv.most_similar(word1)\n",
        "similars2 = model.wv.most_similar(word2)\n",
        "\n",
        "# Print the results\n",
        "print(\"Representation of\", word1, \":\", vector1)\n",
        "print(\"Most similar words to\", word1, \":\", similars1)\n",
        "print(\"Representation of\", word2, \":\", vector2)\n",
        "print(\"Most similar words to\", word2, \":\", similars2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SIr9fFNLxAT2",
        "outputId": "ae5b966c-9b6e-4cd6-f4f2-0cdddbe630a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Representation of push : [ 1.5395569e-03  7.5077750e-03  2.1713087e-03 -4.9668085e-03\n",
            " -4.0774015e-03  6.5845117e-04  9.2415642e-03 -6.3100881e-03\n",
            "  1.9796756e-04 -3.1242380e-03  5.3945067e-03  6.0573299e-03\n",
            " -1.5088018e-03 -5.9175501e-03 -8.6582107e-03 -6.0270345e-03\n",
            " -4.5519830e-03  1.9757783e-03  7.4616699e-03 -7.4353814e-03\n",
            "  9.1637745e-03  3.6634170e-03  4.8392848e-03  9.6576158e-03\n",
            " -1.6779846e-03  6.3166930e-04  3.5842590e-03  2.5750286e-04\n",
            " -4.0177531e-03 -7.8923069e-03 -4.3013571e-03  5.3770570e-03\n",
            "  2.8156540e-03  6.0266140e-03  3.4241064e-04 -5.3579994e-03\n",
            "  2.5592386e-03 -3.9198902e-03 -9.4094248e-03  8.0666235e-03\n",
            " -1.5926086e-03 -1.4773844e-03  4.8250346e-03  8.9691682e-03\n",
            " -8.5496893e-03 -6.5459781e-03 -3.5528501e-03 -5.0315149e-03\n",
            " -9.0100373e-05  6.4891921e-03  7.0794481e-03  1.9165989e-03\n",
            " -5.1965252e-03 -6.2701465e-03  4.8678103e-03 -5.2557187e-03\n",
            "  7.8981783e-04  4.9856156e-03 -5.4350179e-03 -6.5099765e-03\n",
            " -9.2052864e-03 -7.0486232e-03  7.0287781e-03 -7.9072816e-03\n",
            " -5.5270991e-03 -9.3448563e-03  3.5271328e-03 -5.3659370e-03\n",
            " -1.7281657e-03 -7.2139767e-03 -2.4556543e-03 -1.5112506e-03\n",
            " -5.4070097e-04 -1.5431638e-03 -7.9448689e-03  5.4516052e-03\n",
            " -7.2558802e-03 -1.5141525e-03 -4.5608776e-03 -3.9798412e-03\n",
            "  3.4484155e-03  3.5573449e-04 -6.1982591e-03 -6.1731122e-04\n",
            " -3.2091825e-03  4.0899441e-03 -5.4780957e-03  6.8997629e-03\n",
            "  5.2459328e-04  1.2391459e-05 -4.9262838e-03 -6.2768864e-03\n",
            " -5.6361873e-04  9.6364887e-03 -7.3418752e-03 -4.3307515e-03\n",
            "  6.0128234e-03  3.2955380e-03 -4.4248318e-03 -3.7070478e-03]\n",
            "Most similar words to push : [('pulled', 0.1745193749666214), ('woof', 0.0018213242292404175), ('horse', -0.02608865685760975), ('dog', -0.050738248974084854), ('cart', -0.096236452460289), ('say', -0.12427016347646713)]\n",
            "Representation of cat : [-0.00346129  0.00804689  0.00543605  0.00316166  0.00799606  0.00566263\n",
            " -0.00150501 -0.0035149  -0.00212244 -0.00877176 -0.00273138 -0.00293913\n",
            "  0.00731249  0.007774    0.00926104 -0.00343601  0.0038664   0.00266133\n",
            " -0.00515113  0.00739975 -0.00571796  0.00495428 -0.00710672 -0.00786361\n",
            "  0.00440644  0.00052636 -0.00114168  0.00556361 -0.00282513  0.00706699\n",
            "  0.00924345  0.00958251 -0.00444744 -0.00169986  0.0045002   0.00789732\n",
            "  0.00422622  0.00516265  0.00766807 -0.00266104  0.0008509   0.0006954\n",
            "  0.00295165  0.00790368  0.00201655  0.00939877  0.00464271 -0.00169417\n",
            "  0.00162277  0.00844642  0.00237001  0.00419857  0.00322584 -0.00327707\n",
            " -0.0059333  -0.00184374 -0.0069846   0.00652667  0.00420921 -0.00197888\n",
            " -0.00657235  0.00751599  0.00278128  0.00993206 -0.00797804 -0.00510303\n",
            " -0.0083269   0.00489484 -0.00680849  0.00763319 -0.00017041  0.00052339\n",
            " -0.00914959  0.00228107 -0.0054477  -0.00572992  0.00790637  0.00776027\n",
            " -0.0098947   0.00664608  0.00804598  0.00023895  0.00312348  0.00535571\n",
            "  0.0037088  -0.00081767  0.00146904  0.00148119  0.00684261 -0.00519117\n",
            "  0.00514453  0.00624674  0.00594065  0.00615416  0.0065856  -0.00164968\n",
            "  0.00235773  0.00510491  0.00539531  0.00917209]\n",
            "Most similar words to cat : [('horse', 0.1436581015586853), ('cart', 0.11535532772541046), ('dog', 0.09129048883914948), ('say', 0.07637731730937958), ('pulled', 0.05895563215017319), ('woof', -0.2764524817466736)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " I first define the corpus as a list of two lists of words. I then train the FastText model using the FastText() function from Gensim, with a vector size of 100, a window size of 5, and a minimum word count of 1. I also set the sg parameter to 1 to use skip-gram instead of CBOW."
      ],
      "metadata": {
        "id": "BcYDHqk7xYIq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To answer the second part:"
      ],
      "metadata": {
        "id": "L1QKh2SZyFjd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models.fasttext import FastText\n",
        "\n",
        "# Define the corpus\n",
        "corpus = [[\"horse\", \"pulled\", \"cart\"], [\"dog\", \"say\", \"woof\"]]\n",
        "\n",
        "# Train the FastText model\n",
        "model = FastText(corpus, size=100, window=5, min_count=1, workers=4, sg=1)\n",
        "\n",
        "# Get the representations and most similar words for two out-of-vocabulary words\n",
        "word1 = \"giraffe\"\n",
        "word2 = \"cat\"\n",
        "vector1 = model.wv[word1]\n",
        "vector2 = model.wv[word2]\n",
        "similars1 = model.wv.most_similar(word1)\n",
        "similars2 = model.wv.most_similar(word2)\n",
        "\n",
        "# Print the results\n",
        "print(\"Representation of\", word1, \":\", vector1)\n",
        "print(\"Most similar words to\", word1, \":\", similars1)\n",
        "print(\"Representation of\", word2, \":\", vector2)\n",
        "print(\"Most similar words to\", word2, \":\", similars2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "buiJkQtlx9PZ",
        "outputId": "6a8bb95c-e24e-4686-88e6-8376756b3cb2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-27a0841d96c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mword1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"giraffe\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mword2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"cat\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mvector1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mvector2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0msimilars1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, entities)\u001b[0m\n\u001b[1;32m    335\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m             \u001b[0;31m# allow calls like trained_model['office'], as a shorthand for trained_model[['office']]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentity\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mentity\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentities\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mget_vector\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 455\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwords_closer_than\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[0;34m(self, word, use_norm)\u001b[0m\n\u001b[1;32m   1989\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mword_vec\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mngrams_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1990\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# No ngrams of the word are present in self.ngrams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1991\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'all ngrams for word %s absent from model'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1992\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1993\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minit_sims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'all ngrams for word giraffe absent from model'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As can be seen, for the word giraffe i faced an error. The word that Im trying to query is not present in the FastText model vocabulary. This can happen if the word is rare or if the model was trained on a very limited vocabulary. It means that the model is not able to learn a good representation for this word based on the character n-grams in the training corpus."
      ],
      "metadata": {
        "id": "vNvVxM7gyEGx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Use pretrained Word2Vec and FastText models word2vec-google-news-300 and fasttextwiki-\n",
        "news-subwords-300, respectively. Load these models and come up with at least four\n",
        "examples to compare syntactic (2 examples) and semantic (2 examples, hint: find out how to\n",
        "make analogies with Gensim) representations between the two models. Compare and contrast\n",
        "the results."
      ],
      "metadata": {
        "id": "6z-CdCLYQvTH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Syntactic Comparison: Verb Tense and plural Tense"
      ],
      "metadata": {
        "id": "9bXz6ZXT5nVv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "# Load the models\n",
        "w2v_model = api.load('word2vec-google-news-300')\n"
      ],
      "metadata": {
        "id": "uB1wrCcc7_40",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2dfd6c7-40ff-4e76-d674-84ffdc2db28f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader as api\n",
        "ft_model = api.load('fasttext-wiki-news-subwords-300')"
      ],
      "metadata": {
        "id": "LvKDphYZxDTA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare the past and present tense forms of \"run\" in each model\n",
        "w2v_past = w2v_model['ran']\n",
        "w2v_present = w2v_model['run']\n",
        "ft_past = ft_model['ran']\n",
        "ft_present = ft_model['run']\n",
        "\n",
        "# Calculate the cosine similarity between the past and present tense forms of \"run\"\n",
        "w2v_sim = w2v_model.similarity('ran', 'run')\n",
        "ft_sim = ft_model.similarity('ran', 'run')\n",
        "\n",
        "# Print the results\n",
        "print(\"Syntactic Comparison: Verb Tense\")\n",
        "print(\"Cosine similarity between 'ran' and 'run' in Word2Vec:\", w2v_sim)\n",
        "print(\"Cosine similarity between 'ran' and 'run' in FastText:\", ft_sim)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J6g5zEhb0Ifx",
        "outputId": "61375bc8-4374-4176-9369-fdcbd1d27404"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Syntactic Comparison: Verb Tense\n",
            "Cosine similarity between 'ran' and 'run' in Word2Vec: 0.47649786\n",
            "Cosine similarity between 'ran' and 'run' in FastText: 0.7697737\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So as can be seen the similarity between \"run\" and \"ran\" has a higer score inFastText."
      ],
      "metadata": {
        "id": "iICbTOyNxUYZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Compare the singular and plural forms of \"cat\" in each model\n",
        "w2v_sing = w2v_model['cat']\n",
        "w2v_plur = w2v_model['cats']\n",
        "ft_sing = ft_model['cat']\n",
        "ft_plur = ft_model['cats']\n",
        "\n",
        "# Calculate the cosine similarity between the singular and plural forms of \"cat\"\n",
        "w2v_sim = w2v_model.similarity('cat', 'cats')\n",
        "ft_sim = ft_model.similarity('cat', 'cats')\n",
        "\n",
        "# Print the results\n",
        "print(\"Syntactic Comparison: Plurals\")\n",
        "print(\"Cosine similarity between 'cat' and 'cats' in Word2Vec:\", w2v_sim)\n",
        "print(\"Cosine similarity between 'cat' and 'cats' in FastText:\", ft_sim)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u_RP1H6F0YZz",
        "outputId": "7beaf154-9e7b-4a8f-af8d-f1192c652db5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Syntactic Comparison: Plurals\n",
            "Cosine similarity between 'cat' and 'cats' in Word2Vec: 0.8099379\n",
            "Cosine similarity between 'cat' and 'cats' in FastText: 0.8368597\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As can be seen \"cat\" and \"cats\" are similar but the similarity has a higher score in Fasttext again."
      ],
      "metadata": {
        "id": "G_DCpsivxjaB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Semantic Comparison: Country-Capital Relationship/ Synonyms and Antonyms"
      ],
      "metadata": {
        "id": "KCSy3c0f50Vu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the most similar words to the vector difference between \"France\" and \"Paris\" in each model\n",
        "w2v_similar = w2v_model.most_similar(positive=['France', 'capital'], negative=['Paris'])\n",
        "# ft_similar = ft_model.most_similar(positive=['France', 'capital'], negative=['Paris'])\n",
        "\n",
        "# Print the results\n",
        "print(\"Semantic Comparison: Country-Capital Relationship\")\n",
        "print(\"Most similar words to 'France - Paris + capital' in Word2Vec:\", w2v_similar[:5])\n",
        "# print(\"Most similar words to 'France - Paris + capital' in FastText:\", ft_similar[:5])\n"
      ],
      "metadata": {
        "id": "0pCFbnz85xhX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "565f1e54-8ca9-4aa3-e493-794312d992d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Semantic Comparison: Country-Capital Relationship\n",
            "Most similar words to 'France - Paris + capital' in Word2Vec: [('captial', 0.42486903071403503), ('undistributed_profits', 0.4229094982147217), ('invest_ment', 0.4195176959037781), ('worth_##mln_rub', 0.40540462732315063), ('Vietnam_reunifying', 0.3988904058933258)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the most similar words to the vector difference between \"France\" and \"Paris\" in each model\n",
        "# w2v_similar = w2v_model.most_similar(positive=['France', 'capital'], negative=['Paris'])\n",
        "ft_similar = ft_model.most_similar(positive=['France', 'capital'], negative=['Paris'])\n",
        "\n",
        "# Print the results\n",
        "print(\"Semantic Comparison: Country-Capital Relationship\")\n",
        "# print(\"Most similar words to 'France - Paris + capital' in Word2Vec:\", w2v_similar[:5])\n",
        "print(\"Most similar words to 'France - Paris + capital' in FastText:\", ft_similar[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5oMsvdbHrIz8",
        "outputId": "54e83e79-86ca-4392-af53-ccba22613711"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Semantic Comparison: Country-Capital Relationship\n",
            "Most similar words to 'France - Paris + capital' in FastText: [('non-capital', 0.5866502523422241), ('investment', 0.5786392092704773), ('capital-', 0.5744650363922119), ('capital-labour', 0.5675524473190308), ('capital-rich', 0.5655425786972046)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the mentioned examples, I tried to find the word that is most similar to 'Paris' in the same way that 'France' is similar to 'Capital'."
      ],
      "metadata": {
        "id": "hWBjYpGt0AGp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As another example:"
      ],
      "metadata": {
        "id": "7-0gNPSp0tVK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "w2v_result = w2v_model.most_similar(positive=['Berlin', 'France'], negative=['Germany'], topn=1)\n",
        "print(w2v_result)\n",
        "\n",
        "# Syntactic representation using FastText\n",
        "ft_result = ft_model.most_similar(positive=['Berlin', 'France'], negative=['Germany'], topn=1)\n",
        "print(ft_result)"
      ],
      "metadata": {
        "id": "GozA5sgO0f03"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To find the word that is most similar to 'Paris' in the same way that 'Berlin' is similar to 'Germany' we can see that both models perform pretty good but word2vec is better here. So overal the choice of model will depend on the specific task and data being used."
      ],
      "metadata": {
        "id": "P5tM-toL0vRB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare the similarity between the synonyms \"happy\" and \"joyful\" and the antonyms \"happy\" and \"sad\" in each model\n",
        "w2v_sim1 = w2v_model.similarity('happy', 'joyful')\n",
        "w2v_sim2 = w2v_model.similarity('happy', 'sad')\n",
        "ft_sim1 = ft_model.similarity('happy', 'joyful')\n",
        "ft_sim2 = ft_model.similarity('happy', 'sad')\n",
        "\n",
        "# Print the results\n",
        "print(\"Semantic Comparison: Synonyms and Antonyms\")\n",
        "print(\"Cosine similarity between 'happy' and 'joyful' in Word2Vec:\", w2v_sim1)\n",
        "print(\"Cosine similarity between 'happy' and 'joyful' in FastText:\", ft_sim1)\n",
        "print(\"Cosine similarity between 'happy' and 'sad' in Word2Vec:\", w2v_sim2)\n",
        "print(\"Cosine similarity between 'happy' and 'sad' in FastText:\", ft_sim1)\n"
      ],
      "metadata": {
        "id": "YKcOkwzy59K0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40026578-bf09-45a3-9e28-94bb3e88d258"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Semantic Comparison: Synonyms and Antonyms\n",
            "Cosine similarity between 'happy' and 'joyful' in Word2Vec: 0.42381963\n",
            "Cosine similarity between 'happy' and 'joyful' in FastText: 0.71287423\n",
            "Cosine similarity between 'happy' and 'sad' in Word2Vec: 0.5354614\n",
            "Cosine similarity between 'happy' and 'sad' in FastText: 0.71287423\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The semantic similarity between words like \"happy\" and \"joyful\" is more in fasttext. The non-similarity between \"sad\" and \"happy\" is more in FastText. It seems that if two words are similar or very non-similar, fastText assign a higher score in comparision to word2vec."
      ],
      "metadata": {
        "id": "7kRvzXSpyA8S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. What do you think the potential benefits are of using FastText over Word2Vec and vice\n",
        "versa? "
      ],
      "metadata": {
        "id": "e5zUk-w1QyrO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://medium.com/swlh/a-quick-overview-of-the-main-difference-between-word2vec-and-fasttext-b9d3f6e274e9"
      ],
      "metadata": {
        "id": "H5JRDchf9Uya"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://cesconi.com/what-is-the-main-difference-between-word2vec-and-fasttext-57bdaf3a69ef\n"
      ],
      "metadata": {
        "id": "NZMHZmRt-Akt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One of the primary benefits of FastText is its ability to capture subword information, which makes it particularly useful in handling out-of-vocabulary words, especially in languages with complex morphologies or character-level differences. By breaking words down into their constituent subwords, FastText can capture relationships between words that share similar prefixes or suffixes, even if they are not exact matches. This is particularly useful for tasks such as text classification, where subword information can help the model generalize better to unseen words. In contrast, Word2Vec is more suited to tasks that require a larger vocabulary of exact word matches, such as word analogy tasks.\n",
        "\n",
        "Another benefit of FastText is that it can capture polysemy better than Word2Vec, meaning it can distinguish between different senses of a word based on the contexts in which it appears. This is because FastText's subword information can help disambiguate the different meanings of a word, whereas Word2Vec may conflate them if they are represented by the same vector. For example, the word \"bank\" can refer to a financial institution or the side of a river, and FastText can better capture these distinct meanings.\n",
        "\n",
        "On the other hand, Word2Vec has been around longer and has been widely adopted and extensively studied in the NLP community. It has been shown to be effective in a variety of applications, including sentiment analysis, text classification, and machine translation. It is also computationally more efficient than FastText, especially for larger vocabularies, due to its use of a hierarchical softmax."
      ],
      "metadata": {
        "id": "81GzYgIy8wCq"
      }
    }
  ]
}