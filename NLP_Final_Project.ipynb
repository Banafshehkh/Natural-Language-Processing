{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM3Yx0cUf0BfQrmDaCggLsO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Banafshehkh/Natural-Language-Processing/blob/main/NLP_Final_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Author: Banafsheh Khazali, Shokoofa Ghods\n",
        "# Date: March 01, 2023"
      ],
      "metadata": {
        "id": "GkGUoYtjiypU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Semantic Text Similarity on Medical Symptoms** \n"
      ],
      "metadata": {
        "id": "nuXfpHocixuW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FhPj0YJ2z-g5",
        "outputId": "2984b599-19a1-471a-98a1-7eff63409401"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "import os\n",
        "import csv\n"
      ],
      "metadata": {
        "id": "pI24xlZBjYwv"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Scraping the data**"
      ],
      "metadata": {
        "id": "xFj_c5wfjGbX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To scrape the data, We can use Python libraries such as BeautifulSoup and requests. \n",
        "\n"
      ],
      "metadata": {
        "id": "CeDUclrHjQkE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the URL of the symptom checker page\n",
        "url = 'https://www.mayoclinic.org/symptom-checker/select-symptom/itt-20009075'\n",
        "\n",
        "# Send a GET request to the URL\n",
        "response = requests.get(url)\n",
        "\n",
        "# Parse the HTML content of the response using BeautifulSoup\n",
        "soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "# Find the section of the page that contains adult symptoms\n",
        "adult_symptoms_section = soup.find('div', {'class': 'adult'})\n",
        "# Check that the adult symptoms section was found\n",
        "if adult_symptoms_section:\n",
        "    # Find all links within the adult symptoms section\n",
        "    adult_symptom_links = adult_symptoms_section.find_all('a', href=True)\n",
        "\n",
        "    # Extract the URLs from the links\n",
        "    adult_symptom_urls = [link['href'] for link in adult_symptom_links]\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "# Find the section of the page that contains child symptoms\n",
        "child_symptoms_section = soup.find('div', {'class': 'child'})\n",
        "\n",
        "# Check that the child symptoms section was found\n",
        "if child_symptoms_section:\n",
        "    # Find all links within the child symptoms section\n",
        "    child_symptom_links = child_symptoms_section.find_all('a', href=True)\n",
        "\n",
        "    # Extract the URLs from the links\n",
        "    child_symptom_urls = [link['href'] for link in child_symptom_links]\n",
        "\n",
        "\n",
        "\n",
        "# Print the URLs for the adult symptoms\n",
        "full_adult_url = []\n",
        "for i in adult_symptom_urls:\n",
        "  full_adult_url.append(\"https://www.mayoclinic.org/\"+i)\n",
        "\n",
        "print('\\nAdult Symptom URLs:', full_adult_url)\n",
        "\n",
        "# Print the URLs for the child symptoms\n",
        "full_child_urls = []\n",
        "for i in child_symptom_urls:\n",
        "  full_child_urls.append(\"https://www.mayoclinic.org/\"+i)\n",
        "\n",
        "print('\\nChild Symptom URLs:', full_child_urls)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3XoWmDF26joS",
        "outputId": "d4ffe04a-762f-477b-85f3-c830dd80a66f"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Adult Symptom URLs: ['https://www.mayoclinic.org//symptom-checker/abdominal-pain-in-adults-adult/related-factors/itt-20009075', 'https://www.mayoclinic.org//symptom-checker/blood-in-stool-in-adults-adult/related-factors/itt-20009075', 'https://www.mayoclinic.org//symptom-checker/chest-pain-in-adults-adult/related-factors/itt-20009075', 'https://www.mayoclinic.org//symptom-checker/constipation-in-adults-adult/related-factors/itt-20009075', 'https://www.mayoclinic.org//symptom-checker/cough-in-adults-adult/related-factors/itt-20009075', 'https://www.mayoclinic.org//symptom-checker/diarrhea-in-adults-adult/related-factors/itt-20009075', 'https://www.mayoclinic.org//symptom-checker/difficulty-swallowing-in-adults-adult/related-factors/itt-20009075', 'https://www.mayoclinic.org//symptom-checker/dizziness-in-adults-adult/related-factors/itt-20009075', 'https://www.mayoclinic.org//symptom-checker/eye-discomfort-and-redness-in-adults-adult/related-factors/itt-20009075', 'https://www.mayoclinic.org//symptom-checker/eye-problems-in-adults-adult/related-factors/itt-20009075', 'https://www.mayoclinic.org//symptom-checker/foot-pain-or-ankle-pain-in-adults-adult/related-factors/itt-20009075', 'https://www.mayoclinic.org//symptom-checker/foot-swelling-or-leg-swelling-in-adults-adult/related-factors/itt-20009075', 'https://www.mayoclinic.org//symptom-checker/headaches-in-adults-adult/related-factors/itt-20009075', 'https://www.mayoclinic.org//symptom-checker/heart-palpitations-in-adults-adult/related-factors/itt-20009075', 'https://www.mayoclinic.org//symptom-checker/hip-pain-in-adults-adult/related-factors/itt-20009075', 'https://www.mayoclinic.org//symptom-checker/knee-pain-in-adults-adult/related-factors/itt-20009075', 'https://www.mayoclinic.org//symptom-checker/low-back-pain-in-adults-adult/related-factors/itt-20009075', 'https://www.mayoclinic.org//symptom-checker/nasal-congestion-in-adults-adult/related-factors/itt-20009075', 'https://www.mayoclinic.org//symptom-checker/nausea-or-vomiting-in-adults-adult/related-factors/itt-20009075', 'https://www.mayoclinic.org//symptom-checker/neck-pain-in-adults-adult/related-factors/itt-20009075', 'https://www.mayoclinic.org//symptom-checker/numbness-or-tingling-in-hands-in-adults-adult/related-factors/itt-20009075', 'https://www.mayoclinic.org//symptom-checker/pelvic-pain-in-adult-females-adult/related-factors/itt-20009075', 'https://www.mayoclinic.org//symptom-checker/pelvic-pain-in-adult-males-adult/related-factors/itt-20009075', 'https://www.mayoclinic.org//symptom-checker/shortness-of-breath-in-adults-adult/related-factors/itt-20009075', 'https://www.mayoclinic.org//symptom-checker/shoulder-pain-in-adults-adult/related-factors/itt-20009075', 'https://www.mayoclinic.org//symptom-checker/sore-throat-in-adults-adult/related-factors/itt-20009075', 'https://www.mayoclinic.org//symptom-checker/urinary-problems-in-adults-adult/related-factors/itt-20009075', 'https://www.mayoclinic.org//symptom-checker/wheezing-in-adults-adult/related-factors/itt-20009075']\n",
            "\n",
            "Child Symptom URLs: ['https://www.mayoclinic.org//symptom-checker/abdominal-pain-in-children-child/related-factors/itt-20009075', 'https://www.mayoclinic.org//symptom-checker/constipation-in-children-child/related-factors/itt-20009075', 'https://www.mayoclinic.org//symptom-checker/cough-in-children-child/related-factors/itt-20009075', 'https://www.mayoclinic.org//symptom-checker/diarrhea-in-children-child/related-factors/itt-20009075', 'https://www.mayoclinic.org//symptom-checker/ear-problems-in-children-child/related-factors/itt-20009075', 'https://www.mayoclinic.org//symptom-checker/earache-in-children-child/related-factors/itt-20009075', 'https://www.mayoclinic.org//symptom-checker/eye-discomfort-and-redness-in-children-child/related-factors/itt-20009075', 'https://www.mayoclinic.org//symptom-checker/eye-problems-in-children-child/related-factors/itt-20009075', 'https://www.mayoclinic.org//symptom-checker/fever-in-children-child/related-factors/itt-20009075', 'https://www.mayoclinic.org//symptom-checker/headaches-in-children-child/related-factors/itt-20009075', 'https://www.mayoclinic.org//symptom-checker/joint-pain-or-muscle-pain-in-children-child/related-factors/itt-20009075', 'https://www.mayoclinic.org//symptom-checker/nasal-congestion-in-children-child/related-factors/itt-20009075', 'https://www.mayoclinic.org//symptom-checker/nausea-or-vomiting-in-children-child/related-factors/itt-20009075', 'https://www.mayoclinic.org//symptom-checker/skin-rashes-in-children-child/related-factors/itt-20009075', 'https://www.mayoclinic.org//symptom-checker/sore-throat-in-children-child/related-factors/itt-20009075', 'https://www.mayoclinic.org//symptom-checker/urinary-problems-in-children-child/related-factors/itt-20009075', 'https://www.mayoclinic.org//symptom-checker/wheezing-in-children-child/related-factors/itt-20009075']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**extract adult symptoms**"
      ],
      "metadata": {
        "id": "Ub5SRHtkmyK5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loop over the URLs and extract the relevant information\n",
        "for i, url in enumerate(full_adult_url):\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "    adult_symptoms_section = soup.find('div', {'id': 'main-content'})\n",
        "    \n",
        "    if adult_symptoms_section is not None:\n",
        "        filename = f'adult_page{i+1}.txt'\n",
        "        with open(filename, 'w') as f:\n",
        "            f.write(adult_symptoms_section.text)\n",
        "    else:\n",
        "        print(f\"No 'adult' section found on page {i+1}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y2BQivmGqvsz",
        "outputId": "663be18c-57c1-4125-c6d2-52cc01d996cd"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No 'adult' section found on page 4\n",
            "No 'adult' section found on page 14\n",
            "No 'adult' section found on page 16\n",
            "No 'adult' section found on page 18\n",
            "No 'adult' section found on page 19\n",
            "No 'adult' section found on page 22\n",
            "No 'adult' section found on page 24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**extract children symptoms**"
      ],
      "metadata": {
        "id": "iONWhbfdm3vw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loop over the URLs and extract the relevant information\n",
        "for i, url in enumerate(full_child_urls):\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "    child_symptoms_section = soup.find('div', {'id': 'main-content'})\n",
        "    \n",
        "    if child_symptoms_section is not None:\n",
        "        filename = f'child_page{i+1}.txt'\n",
        "        with open(filename, 'w') as f:\n",
        "            f.write(child_symptoms_section.text)\n",
        "    else:\n",
        "        print(f\"No 'child' section found on page {i+1}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ulFQYs1Gsj8d",
        "outputId": "3a395bdd-f0c0-47c9-f3c4-b63610ac617c"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No 'child' section found on page 2\n",
            "No 'child' section found on page 3\n",
            "No 'child' section found on page 4\n",
            "No 'child' section found on page 9\n",
            "No 'child' section found on page 12\n",
            "No 'child' section found on page 13\n",
            "No 'child' section found on page 15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Preprocessing the data**"
      ],
      "metadata": {
        "id": "CTzoSVMRkDLl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To preprocess the data, We may need to clean and tokenize the text, remove stop words, and perform stemming or lemmatization."
      ],
      "metadata": {
        "id": "qIeSnso4kLWz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir adult"
      ],
      "metadata": {
        "id": "mmVMGeM2xvUV"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir children"
      ],
      "metadata": {
        "id": "iK9XFW2zx6--"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preprocess adult files**"
      ],
      "metadata": {
        "id": "FWEUTCuJ2TUk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess(text):\n",
        "    # Tokenize the text\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    \n",
        "    # Remove stop words\n",
        "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
        "    \n",
        "    # Lemmatize the tokens\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
        "    \n",
        "    return lemmatized_tokens\n",
        "\n",
        "# Define the directory where the text files are located\n",
        "adult_dir_in = '/content/adult'\n",
        "adult_dir_out = '/content/adult/pre_adult'\n",
        "\n",
        "# Create the output directory if it doesn't exist\n",
        "if not os.path.exists(adult_dir_out):\n",
        "    os.makedirs(adult_dir_out)\n",
        "\n",
        "\n",
        "# Loop over the input files\n",
        "for filename in os.listdir(adult_dir_in):\n",
        "    if filename.endswith('.txt'):\n",
        "        # Read the input file\n",
        "        with open(os.path.join(adult_dir_in, filename), 'r') as f:\n",
        "            text = f.read()\n",
        "        \n",
        "        # Preprocess the text\n",
        "        tokens = word_tokenize(text.lower())\n",
        "        filtered_tokens = [token for token in tokens if token not in stop_words]\n",
        "        lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
        "        preprocessed_text = ' '.join(lemmatized_tokens)\n",
        "        \n",
        "        # Write the preprocessed text to a new file\n",
        "        output_filename = os.path.join(adult_dir_out, filename)\n",
        "        with open(output_filename, 'w') as f:\n",
        "            f.write(preprocessed_text)\n",
        "            \n",
        "        print(f'Processed file: {filename}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bDdJkIc5xfBc",
        "outputId": "af5df739-09a5-46ad-98e8-d8e416d95ae7"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed file: adltu_page2.txt\n",
            "Processed file: adltu_page1.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preprocess children files**"
      ],
      "metadata": {
        "id": "_I6It03A2iH8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess(text):\n",
        "    # Tokenize the text\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    \n",
        "    # Remove stop words\n",
        "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
        "    \n",
        "    # Lemmatize the tokens\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
        "    \n",
        "    return lemmatized_tokens\n",
        "\n",
        "# Define the directory where the text files are located\n",
        "child_dir_in = '/content/children'\n",
        "child_dir_out = '/content/children/pre_child'\n",
        "\n",
        "# Create the output directory if it doesn't exist\n",
        "if not os.path.exists(child_dir_out):\n",
        "    os.makedirs(child_dir_out)\n",
        "\n",
        "\n",
        "# Loop over the input files\n",
        "for filename in os.listdir(child_dir_in):\n",
        "    if filename.endswith('.txt'):\n",
        "        # Read the input file\n",
        "        with open(os.path.join(child_dir_in, filename), 'r') as f:\n",
        "            text = f.read()\n",
        "        \n",
        "        # Preprocess the text\n",
        "        tokens = word_tokenize(text.lower())\n",
        "        filtered_tokens = [token for token in tokens if token not in stop_words]\n",
        "        lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
        "        preprocessed_text = ' '.join(lemmatized_tokens)\n",
        "        \n",
        "        # Write the preprocessed text to a new file\n",
        "        output_filename = os.path.join(child_dir_out, filename)\n",
        "        with open(output_filename, 'w') as f:\n",
        "            f.write(preprocessed_text)\n",
        "            \n",
        "        print(f'Processed file: {filename}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YFiz6u1EkGSq",
        "outputId": "e35c9890-4e4d-4166-c8e4-80ec173cde38"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed file: child_page17.txt\n",
            "Processed file: child_page16.txt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Creating the document corpus**"
      ],
      "metadata": {
        "id": "zqBTx8yHkZcu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To create the document corpus for each disease, we can collect diverse sets of factors that contribute to that pain as a document and then combine these documents to create the corpus. "
      ],
      "metadata": {
        "id": "ZHCCCj1skeim"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "docs = [\n",
        "    'activity or overuse, long period of rest, joint deformity',\n",
        "    'movement, prolonged sitting or standing, joint weakness'\n",
        "]\n",
        "\n",
        "corpus = []\n",
        "\n",
        "for doc in docs:\n",
        "    tokens = preprocess(doc)\n",
        "    corpus.append(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "XqqXe6MzkV8i",
        "outputId": "0d384ed5-2813-43ca-98c2-f2c8a6517269"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "LookupError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     83\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m                     \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{self.subdir}/{zip_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93momw-1.4\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('omw-1.4')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/omw-1.4.zip/omw-1.4/\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-0a1317a5f219>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-403f5dabc9b9>\u001b[0m in \u001b[0;36mpreprocess\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# Lemmatize the tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mlemmatized_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlemmatizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfiltered_tokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlemmatized_tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-403f5dabc9b9>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# Lemmatize the tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mlemmatized_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlemmatizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfiltered_tokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlemmatized_tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/nltk/stem/wordnet.py\u001b[0m in \u001b[0;36mlemmatize\u001b[0;34m(self, word, pos)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mlemma\u001b[0m \u001b[0mof\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mword\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mgiven\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \"\"\"\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mlemmas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_morphy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlemmas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlemmas\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m         \u001b[0;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;31m# __class__ to something new:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;31m# Load the corpus.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0mcorpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__reader_cls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;31m# This is where the magic happens!  Transform ourselves into\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/nltk/corpus/reader/wordnet.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, omw_reader)\u001b[0m\n\u001b[1;32m   1174\u001b[0m             )\n\u001b[1;32m   1175\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1176\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprovenances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0momw_prov\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1178\u001b[0m         \u001b[0;31m# A cache to store the wordnet data of multiple languages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/nltk/corpus/reader/wordnet.py\u001b[0m in \u001b[0;36momw_prov\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1283\u001b[0m         \u001b[0mprovdict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1284\u001b[0m         \u001b[0mprovdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"eng\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1285\u001b[0;31m         \u001b[0mfileids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_omw_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1286\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfileid\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfileids\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1287\u001b[0m             \u001b[0mprov\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlangfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m         \u001b[0;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;31m# __class__ to something new:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     84\u001b[0m                     \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{self.subdir}/{zip_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;31m# Load the corpus.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                 \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{self.subdir}/{self.__name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"*\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93momw-1.4\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('omw-1.4')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/omw-1.4\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# define the URL of the Mayo Clinic Symptom Checker\n",
        "url = \"https://www.mayoclinic.org/symptom-checker/select-symptom/itt-20009075\"\n",
        "\n",
        "# send a GET request to the website and store the response\n",
        "response = requests.get(url)\n",
        "\n",
        "# create a BeautifulSoup object from the response text\n",
        "soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "# find the section of the website that contains the adult symptoms\n",
        "adult_symptoms_section = soup.find(\"section\", id=\"adultSymptomsSection\")\n",
        "\n",
        "# find all the links to adult symptoms\n",
        "adult_symptom_links = adult_symptoms_section.find_all(\"a\")\n"
      ],
      "metadata": {
        "id": "VL_wBZYXktLv",
        "outputId": "0f88b9ec-23f2-4301-981d-f0ebaa342d4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-e9577066a016>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# find all the links to adult symptoms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0madult_symptom_links\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madult_symptoms_section\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"a\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_all'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WOHGHGa6zwlZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}